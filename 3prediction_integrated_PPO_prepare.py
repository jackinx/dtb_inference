# -*- coding: utf-8 -*-
"""
FEDformer PPO ë°ì´í„° ìƒì„± ë° ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ (v22.1 - ë²„ê·¸ ìˆ˜ì •)

ì£¼ìš” ê¸°ëŠ¥ (v22 ëŒ€ë¹„ ê°œì„ ):
1.  [ë²„ê·¸ ìˆ˜ì •] pandas Seriesì—ì„œ ë§ˆì§€ë§‰ ë‚ ì§œë¥¼ ê°€ì ¸ì˜¬ ë•Œ ë°œìƒí•˜ë˜ KeyErrorë¥¼ .iloc[-1]ì„ ì‚¬ìš©í•˜ì—¬ í•´ê²°.
2.  [ì•ˆì •ì„± ê°•í™”] í•„í„°ë§ëœ ë‚ ì§œ Seriesì—ì„œë„ .ilocë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì ì¸ ì¸ë±ì‹± ë³´ì¥.
"""
import os
import sys
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import sqlite3
import gc
import json
import joblib
import copy
from pathlib import Path
from types import SimpleNamespace
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, mean_squared_error
from tqdm import tqdm
from datetime import datetime, timedelta
import multiprocessing as mp
from typing import List, Dict, Any, Tuple, Optional
from contextlib import contextmanager
import matplotlib
from matplotlib import font_manager as fm # <--- font_manager import
matplotlib.use("Agg")
import matplotlib.pyplot as plt




# ==================================================
# 1. ê²½ë¡œ ë° ëª¨ë“ˆ ì„¤ì •
# ==================================================
try:
    PROJECT_ROOT = Path(__file__).resolve().parent
except NameError:
    PROJECT_ROOT = Path(os.getcwd())

FEDFORMER_ROOT = PROJECT_ROOT / 'FEDformer-master'
if str(PROJECT_ROOT) not in sys.path: sys.path.insert(0, str(PROJECT_ROOT))
if str(FEDFORMER_ROOT) not in sys.path: sys.path.insert(0, str(FEDFORMER_ROOT))
try:
    from FEDformer import Model as FEDformer_base
    from utils.timefeatures import time_features
except ImportError as e:
    sys.exit(f"âŒ CRITICAL ERROR: FEDformer ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

# ==================================================
# 2. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
# ==================================================
class FEDformerWithEmbedding(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = copy.deepcopy(config)
        self.ticker_embedding = nn.Embedding(self.config.num_classes, self.config.embedding_dim)
        self.embedding_dropout = nn.Dropout(self.config.embedding_dropout)
        config_for_base = copy.deepcopy(self.config)
        config_for_base.enc_in += self.config.embedding_dim
        config_for_base.dec_in += self.config.embedding_dim
        self.base_model = FEDformer_base(config_for_base)
    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, ticker_ids):
        emb = self.ticker_embedding(ticker_ids.squeeze(-1))
        emb = self.embedding_dropout(emb)
        emb_enc = emb.unsqueeze(1).repeat(1, self.config.seq_len, 1)
        emb_dec = emb.unsqueeze(1).repeat(1, self.config.label_len + self.config.pred_len, 1)
        x_enc_with_emb = torch.cat([x_enc, emb_enc], dim=-1)
        x_dec_with_emb = torch.cat([x_dec, emb_dec], dim=-1)
        return self.base_model(x_enc_with_emb, x_mark_enc, x_dec_with_emb, x_mark_dec)

# ==================================================
# 3. ì„¤ì • í´ë˜ìŠ¤ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ==================================================
class Config:
    def __init__(self):
        self.PROJECT_ROOT = PROJECT_ROOT
        self.V13_RESULT_DIR = self.PROJECT_ROOT / "training_results_v13"
        self.V14_RESULT_DIR = self.PROJECT_ROOT / "training_results_v14_retrain"
        self.PROCESSED_DATA_DIR = self.PROJECT_ROOT / "processed_data_v9"
        
        self.PROCESSED_DB_PATH = self.PROCESSED_DATA_DIR / "processed_stock_data.db"
        self.METADATA_DIR = self.PROCESSED_DATA_DIR / "metadata"
        self.SCALER_DIR = self.PROCESSED_DATA_DIR / "scalers"
        
        # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ì´ ë¶€ë¶„ ì „ì²´ ìˆ˜ì • â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
        # 1. ì‹¤í–‰ ì‹œì  ê¸°ì¤€ì˜ ê³ ìœ í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 2. PPO ì˜ˆì¸¡ ê²°ê³¼ ìƒìœ„ í´ë”
        predictions_base_dir = self.PROJECT_ROOT / "predictions_v22_ppo"
        
        # 3. íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì´ë¦„ìœ¼ë¡œ í•˜ëŠ” ì‹¤í–‰ ê²°ê³¼ í´ë” ìƒì„±
        self.RUN_OUTPUT_DIR = predictions_base_dir / run_timestamp
        self.RUN_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        
        # 4. ê° ê²°ê³¼ë¬¼ì˜ ê²½ë¡œë¥¼ ìƒˆë¡œìš´ í´ë” ê¸°ì¤€ìœ¼ë¡œ ì¬ì„¤ì •
        self.BACKTEST_PLOTS_DIR = self.RUN_OUTPUT_DIR / "backtest_plots"
        self.BACKTEST_PLOTS_DIR.mkdir(exist_ok=True)
        
        # ì—‘ì…€ íŒŒì¼ì€ í´ë”ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ìˆìœ¼ë¯€ë¡œ íŒŒì¼ëª…ì€ ë‹¨ìˆœí•˜ê²Œ ë³€ê²½
        self.RECOMMENDATION_EXCEL_PATH = self.RUN_OUTPUT_DIR / "final_recommendations.xlsx"
        
        # ì°¸ê³ : ppo_predictions.dbëŠ” ëª¨ë“  ì‹¤í–‰ ê²°ê³¼ë¥¼ ëˆ„ì í•˜ëŠ” ìš©ë„ì´ë¯€ë¡œ
        # íƒ€ì„ìŠ¤íƒ¬í”„ í´ë” ë°–, ìƒìœ„ í´ë”ì— ìœ„ì¹˜ì‹œí‚¤ëŠ” ê²ƒì´ ë” ì í•©í•©ë‹ˆë‹¤.
        self.PPO_DB_PATH = predictions_base_dir / "ppo_predictions.db"
        # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² ì—¬ê¸°ê¹Œì§€ ì „ì²´ ìˆ˜ì • â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
        
        
        source_project_root = self.PROJECT_ROOT.parent / "DeepTrader_baekdoosan"
        if not source_project_root.exists():
            source_project_root = Path(r"C:\Users\jacki\OneDrive\Documents\anaconda_projects\DeepTrader_baekdoosan")
        self.SOURCE_DB_PATH = source_project_root / "stock_data.db"
        
        self.MAX_CONCURRENT_PROCESSES = max(1, mp.cpu_count() // 2)
        self.DB_TIMEOUT = 30
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        self.RECOMMEND_COUNT_BULL = 15
        self.RECOMMEND_COUNT_NEUTRAL = 7
        self.RECOMMEND_COUNT_BEAR = 3
        self.MARKET_UP_THRESHOLD = 0.2
        self.MARKET_DOWN_THRESHOLD = -0.3
        
        self.MIN_AVG_VOLUME = 100000
        self.MAX_PER_SECTOR = 3

MODEL_CONFIGS = {
    'base': SimpleNamespace(config_name='base', version='Fourier', model_name='FEDformer', seq_len=60, label_len=30, pred_len=1, moving_avg=25, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff_multiplier=4, dropout=0.1, activation='gelu', output_attention=False, modes=64, mode_select='random', L=3, base='Fourier', cross_activation='tanh', embed='timeF', freq='d'),
    'tuning_v1': SimpleNamespace(config_name='tuning_v1', version='Fourier', model_name='FEDformer', seq_len=60, label_len=30, pred_len=1, moving_avg=25, d_model=256, n_heads=8, e_layers=2, d_layers=1, d_ff_multiplier=4, dropout=0.2, activation='gelu', output_attention=False, modes=64, mode_select='random', L=3, base='Fourier', cross_activation='tanh', embed='timeF', freq='d'),
    'shallow_wide': SimpleNamespace(config_name='shallow_wide', version='Fourier', model_name='FEDformer', seq_len=60, label_len=30, pred_len=1, moving_avg=25, d_model=384, n_heads=12, e_layers=1, d_layers=1, d_ff_multiplier=3, dropout=0.35, activation='gelu', output_attention=False, modes=48, mode_select='random', L=3, base='Fourier', cross_activation='tanh', embed='timeF', freq='d'),
    'deep_narrow': SimpleNamespace(config_name='deep_narrow', version='Fourier', model_name='FEDformer', seq_len=60, label_len=30, pred_len=1, moving_avg=25, d_model=256, n_heads=8, e_layers=4, d_layers=2, d_ff_multiplier=4, dropout=0.2, activation='gelu', output_attention=False, modes=64, mode_select='random', L=4, base='Fourier', cross_activation='tanh', embed='timeF', freq='d'),
    'high_freq': SimpleNamespace(config_name='high_freq', version='Fourier', model_name='FEDformer', seq_len=40, label_len=20, pred_len=1, moving_avg=15, d_model=320, n_heads=10, e_layers=2, d_layers=1, d_ff_multiplier=4, dropout=0.25, activation='gelu', output_attention=False, modes=96, mode_select='random', L=3, base='Fourier', cross_activation='tanh', embed='timeF', freq='d'),
    'low_freq': SimpleNamespace(config_name='low_freq', version='Fourier', model_name='FEDformer', seq_len=80, label_len=40, pred_len=1, moving_avg=35, d_model=384, n_heads=8, e_layers=2, d_layers=1, d_ff_multiplier=4, dropout=0.15, activation='gelu', output_attention=False, modes=32, mode_select='low', L=3, base='Fourier', cross_activation='tanh', embed='timeF', freq='d'),
    'regularized': SimpleNamespace(config_name='regularized', version='Fourier', model_name='FEDformer', seq_len=60, label_len=30, pred_len=1, moving_avg=25, d_model=192, n_heads=6, e_layers=2, d_layers=1, d_ff_multiplier=2, dropout=0.4, activation='gelu', output_attention=False, modes=40, mode_select='random', L=3, base='Fourier', cross_activation='tanh', embed='timeF', freq='d')
}
for cfg_item in MODEL_CONFIGS.values():
    cfg_item.d_ff = int(cfg_item.d_model * cfg_item.d_ff_multiplier)
    cfg_item.embedding_dim=16; cfg_item.embedding_dropout=0.2

# ==================================================
# 4. ë°ì´í„° ë° ìœ í‹¸ë¦¬í‹°
# ==================================================
@contextmanager
def suppress_stdout():
    """ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ë°œìƒí•˜ëŠ” print ì¶œë ¥ì„ ìˆ¨ê¹ë‹ˆë‹¤."""
    with open(os.devnull, "w") as devnull:
        old_stdout = sys.stdout
        sys.stdout = devnull
        try:
            yield
        finally:
            sys.stdout = old_stdout

class DataHandler:
    def __init__(self, processed_db_path: Path, source_db_path: Path, timeout: int):
        if not processed_db_path.exists() or not source_db_path.exists():
            sys.exit(f"âŒ DB íŒŒì¼ ì˜¤ë¥˜: DB ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\n- {processed_db_path}\n- {source_db_path}")
        self.processed_db_path = processed_db_path
        self.source_db_path = source_db_path
        self.timeout = timeout
    def get_stock_metadata(self) -> Dict[str, Dict[str, str]]:
        with sqlite3.connect(self.source_db_path, timeout=self.timeout) as conn:
            try:
                df = pd.read_sql_query("SELECT ticker, name, market, sector FROM stocks", conn)
                df['ticker'] = df['ticker'].astype(str) # <--- ì´ ì¤„ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.
            except (pd.errors.DatabaseError, sqlite3.OperationalError) as e:
                if 'no such column: sector' in str(e):
                    print("  - ê²½ê³ : 'stocks' í…Œì´ë¸”ì— 'sector' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì„¹í„° ë¶„ì‚° ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
                    df = pd.read_sql_query("SELECT ticker, name, market FROM stocks", conn)
                else: raise e
        return df.set_index('ticker').to_dict('index')
    def get_all_trading_dates(self) -> pd.Series:
        """[ìˆ˜ì •] ë°˜í™˜ íƒ€ì…ì„ DatetimeIndexì—ì„œ Seriesë¡œ ëª…ì‹œ"""
        with sqlite3.connect(self.processed_db_path, timeout=self.timeout) as conn:
            df = pd.read_sql_query("SELECT DISTINCT date FROM processed_daily_prices ORDER BY date", conn)
        return pd.to_datetime(df['date'])
    # 6prediction_integrated_PPO_prepare.py íŒŒì¼ì˜ DataHandler í´ë˜ìŠ¤

    def get_price_data_until(self, ticker: str, end_date: str, num_rows: int) -> Optional[pd.DataFrame]:
        with sqlite3.connect(self.processed_db_path, timeout=self.timeout) as conn:
            # [ìˆ˜ì •] date ì»¬ëŸ¼ì„ ëª…ì‹œì ìœ¼ë¡œ date íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
            query = f"SELECT * FROM processed_daily_prices WHERE ticker = ? AND date(date) <= ? ORDER BY date DESC LIMIT ?"
            df = pd.read_sql_query(query, conn, params=(ticker, end_date, num_rows))
        if df.empty or len(df) < num_rows: return None
        df['date'] = pd.to_datetime(df['date'])
        return df.sort_values('date', ascending=True).reset_index(drop=True)
        
        
    # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ì´ ë¶€ë¶„ì„ ì¶”ê°€ â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
    def get_avg_volume(self, ticker: str, days: int = 20) -> float:
        """ì§€ì •ëœ ì¢…ëª©ì˜ ìµœê·¼ Nì¼ í‰ê·  ê±°ë˜ëŸ‰ì„ ì›ë³¸ DBì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."""
        # ë³€í™˜ë˜ì§€ ì•Šì€ ì›ë³¸ ê±°ë˜ëŸ‰ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ source_db_pathë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        with sqlite3.connect(self.source_db_path, timeout=self.timeout) as conn:
            try:
                # í•´ë‹¹ ì¢…ëª©ì˜ ë°ì´í„°ë¥¼ ë‚ ì§œ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìµœê·¼ `days`ê°œ ë§Œí¼ ì¡°íšŒ
                query = f"SELECT volume FROM daily_prices WHERE ticker = ? ORDER BY date DESC LIMIT ?"
                df = pd.read_sql_query(query, conn, params=(ticker, days))
                
                # ë°ì´í„°ê°€ ì¡´ì¬í•˜ë©´ ê±°ë˜ëŸ‰ì˜ í‰ê· ì„ ê³„ì‚°í•˜ê³ , ì—†ìœ¼ë©´ 0ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
                if not df.empty:
                    return float(df['volume'].mean())
                else:
                    return 0.0
            except Exception as e:
                # í˜¹ì‹œ ëª¨ë¥¼ ì˜¤ë¥˜ ë°œìƒ ì‹œ ê²½ê³ ë¥¼ ì¶œë ¥í•˜ê³  0ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
                print(f"Warning: Could not get avg volume for {ticker}. Error: {e}")
                return 0.0
                

def get_next_trading_day(last_date: datetime, all_trading_dates: pd.Series) -> datetime:
    future_dates = all_trading_dates[all_trading_dates > last_date]
    return future_dates.iloc[0] if not future_dates.empty else last_date + pd.tseries.offsets.BDay(1)

def inverse_transform_price(scaler: StandardScaler, scaled_value: float, close_idx: int) -> float:
    dummy_array = np.zeros((1, scaler.n_features_in_))
    dummy_array[0, close_idx] = scaled_value
    unscaled_log_price = scaler.inverse_transform(dummy_array)[0, close_idx]
    return np.expm1(unscaled_log_price)

# ==================================================
# 5. ì˜ˆì¸¡ ì‹¤í–‰ ë¡œì§
# ==================================================
def run_inference(ticker: str, model_info: Dict, df_recent: pd.DataFrame, next_trading_day: datetime, cfg: Config) -> Optional[float]:
    try:
        config_name = model_info['config_name']
        model_path = model_info['model_path']
        if not model_path.exists(): return None
        
        with suppress_stdout():
            model_config = copy.deepcopy(MODEL_CONFIGS[config_name])
            with open(cfg.METADATA_DIR / f"{ticker}.json", 'r') as f: features = json.load(f)['features']
            model_config.enc_in = model_config.dec_in = len(features)
            model_config.c_out = 1
            model = FEDformer_base(model_config).to(cfg.device)
            model.load_state_dict(torch.load(model_path, map_location=cfg.device, weights_only=False)['model_state_dict'])
            model.eval()

        scaler = joblib.load(cfg.SCALER_DIR / f"{ticker}.pkl")
        data_to_scale = df_recent[features].astype(np.float32)
        scaled_data = scaler.transform(data_to_scale)
        time_marks = time_features(pd.DatetimeIndex(df_recent['date']), freq=model_config.freq).transpose()
        x_enc = torch.tensor(scaled_data, dtype=torch.float32).unsqueeze(0).to(cfg.device)
        x_mark_enc = torch.tensor(time_marks, dtype=torch.float32).unsqueeze(0).to(cfg.device)
        dec_inp_context = x_enc[:, -model_config.label_len:, :]
        dec_inp_zeros = torch.zeros(1, model_config.pred_len, model_config.dec_in, device=cfg.device)
        x_dec = torch.cat([dec_inp_context, dec_inp_zeros], dim=1)
        decoder_dates = pd.DatetimeIndex(df_recent['date'].iloc[-model_config.label_len:].tolist() + [next_trading_day])
        x_mark_dec = torch.tensor(time_features(decoder_dates, freq=model_config.freq).T, dtype=torch.float32).unsqueeze(0).to(cfg.device)
        with torch.no_grad():
            pred_scaled = model(x_enc, x_mark_enc, x_dec, x_mark_dec)[0, -1, 0].item()
        close_idx = features.index('close')
        return inverse_transform_price(scaler, pred_scaled, close_idx)
    except Exception:
        return None
# 6prediction_integrated_PPO_prepare.py íŒŒì¼

# ì´ í•¨ìˆ˜ ì „ì²´ë¥¼ ì•„ë˜ ë‚´ìš©ìœ¼ë¡œ êµì²´í•´ì£¼ì„¸ìš”.
def run_backtest_and_visualize(
    ticker: str, 
    ticker_name: str, 
    model_info: Dict, 
    all_trading_dates: pd.Series, 
    cfg: Config, 
    next_day_prediction: float, # <--- ì´ ì¸ìê°€ ëˆ„ë½ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
    backtest_days: int = 20
) -> Dict:
    """ì¶”ì²œëœ ì¢…ëª©ì˜ ê³¼ê±° Nì¼ê°„ ì˜ˆì¸¡ + ë¯¸ë˜ 1ì¼ ì˜ˆì¸¡ì„ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    
    data_handler = DataHandler(cfg.PROCESSED_DB_PATH, cfg.SOURCE_DB_PATH, cfg.DB_TIMEOUT)
    model_config = MODEL_CONFIGS[model_info['config_name']]
    
    end_date = all_trading_dates.iloc[-1]
    backtest_dates = all_trading_dates[all_trading_dates <= end_date].iloc[-backtest_days:]
    
    predictions, actuals, last_closes = [], [], []
    
    for date in backtest_dates:
        end_of_window = date - pd.Timedelta(days=1)
        df_recent = data_handler.get_price_data_until(ticker, end_of_window.strftime('%Y-%m-%d'), model_config.seq_len)
        
        if df_recent is None or len(df_recent) < model_config.seq_len:
            continue
            
        df_actual = data_handler.get_price_data_until(ticker, date.strftime('%Y-%m-%d'), 1)
        if df_actual is None or df_actual.empty:
            continue
        
        with open(cfg.METADATA_DIR / f"{ticker}.json") as f: features = json.load(f)['features']
        close_idx = features.index('close')
        
        predicted_price = run_inference(ticker, model_info, df_recent, date, cfg)
        if predicted_price is None:
            continue
        
        actual_price_log = df_actual.iloc[0][features].iloc[close_idx]
        last_close_log = df_recent.iloc[-1][features].iloc[close_idx]
        
        predictions.append(predicted_price)
        actuals.append(np.expm1(actual_price_log))
        last_closes.append(np.expm1(last_close_log))

    if not predictions:
        return {'backtest_f1': 0, 'backtest_nrmse': 1.0, 'plot_path': 'N/A'}
    
    y_true, y_pred, y_last = np.array(actuals), np.array(predictions), np.array(last_closes)
    true_dir, pred_dir = (y_true > y_last).astype(int), (y_pred > y_last).astype(int)
    f1 = f1_score(true_dir, pred_dir, zero_division=0)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    nrmse = rmse / (y_true.max() - y_true.min()) if (y_true.max() - y_true.min()) > 1e-5 else 0

    try:
        last_backtest_date = backtest_dates.iloc[-1]
        next_pred_date = get_next_trading_day(last_backtest_date, all_trading_dates)
        plot_dates = backtest_dates[-len(y_pred):].tolist() + [next_pred_date]
        plot_predictions = y_pred.tolist() + [next_day_prediction]

        plt.style.use('seaborn-v0_8-darkgrid')
        plt.rc('font', family='Malgun Gothic' if sys.platform.startswith('win') else 'AppleGothic')
        plt.rcParams['axes.unicode_minus'] = False

        fig, ax = plt.subplots(figsize=(12, 6))
        ax.plot(backtest_dates[-len(y_true):], y_true, label='ì‹¤ì œ ì£¼ê°€ (Actual Price)', marker='o', markersize=4, linestyle='-')
        ax.plot(plot_dates, plot_predictions, label='ì˜ˆì¸¡ ì£¼ê°€ (Predicted Price)', marker='x', markersize=4, linestyle='--')
        ax.set_title(f'ìµœê·¼ {backtest_days}ì¼ ë°±í…ŒìŠ¤íŠ¸: {ticker_name} ({ticker})', fontsize=16)
        ax.set_ylabel('ì¢…ê°€ (KRW)', fontsize=12)
        ax.legend(fontsize=12)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plot_path = cfg.BACKTEST_PLOTS_DIR / f"{ticker}_backtest.png"
        plt.savefig(plot_path)
        plt.close(fig)

    except Exception as e:
        print(f"Plotting Error for {ticker}: {e}")
        plot_path = "N/A"

    return {'backtest_f1': round(f1, 4), 'backtest_nrmse': round(nrmse, 4), 'plot_path': str(plot_path)}

'''
def run_backtest_and_visualize(
    ticker: str, 
    ticker_name: str, 
    model_info: Dict, 
    all_trading_dates: pd.Series, 
    cfg: Config, 
    backtest_days: int = 20
) -> Dict:
    """ì¶”ì²œëœ ì¢…ëª©ì˜ ê³¼ê±° Nì¼ê°„ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ì„±ëŠ¥ ê³„ì‚° ë° ì‹œê°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤."""
    
    data_handler = DataHandler(cfg.PROCESSED_DB_PATH, cfg.SOURCE_DB_PATH, cfg.DB_TIMEOUT)
    model_config = MODEL_CONFIGS[model_info['config_name']]
    
    # ë°±í…ŒìŠ¤íŒ… ê¸°ê°„ ì„¤ì •
    end_date = all_trading_dates.iloc[-1]
    backtest_dates = all_trading_dates[all_trading_dates <= end_date].iloc[-backtest_days:]
    
    predictions, actuals, last_closes = [], [], []
    
    # ê³¼ê±° Nì¼ì— ëŒ€í•´ í•˜ë£¨ì”© ì˜ˆì¸¡ ìˆ˜í–‰
    for date in backtest_dates:
        # ì˜ˆì¸¡ ëŒ€ìƒì¼(date)ì˜ ì „ë‚ ê¹Œì§€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
        end_of_window = date - pd.Timedelta(days=1)
        df_recent = data_handler.get_price_data_until(ticker, end_of_window.strftime('%Y-%m-%d'), model_config.seq_len)
        
        if df_recent is None or len(df_recent) < model_config.seq_len:
            continue
            
        # ì‹¤ì œ ê°’(target) ê°€ì ¸ì˜¤ê¸°
        df_actual = data_handler.get_price_data_until(ticker, date.strftime('%Y-%m-%d'), 1)
        if df_actual is None or df_actual.empty:
            continue
        
        with open(cfg.METADATA_DIR / f"{ticker}.json") as f: features = json.load(f)['features']
        close_idx = features.index('close')
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predicted_price = run_inference(ticker, model_info, df_recent, date, cfg)
        if predicted_price is None:
            continue
        
        # ê²°ê³¼ ì €ì¥ (ë¡œê·¸ ë³€í™˜ëœ ê°’ -> ì›ë˜ ê°€ê²©ìœ¼ë¡œ ë³€í™˜)
        actual_price_log = df_actual.iloc[0][features].iloc[close_idx]
        last_close_log = df_recent.iloc[-1][features].iloc[close_idx]
        
        predictions.append(predicted_price)
        actuals.append(np.expm1(actual_price_log))
        last_closes.append(np.expm1(last_close_log))

    if not predictions:
        return {'backtest_f1': 0, 'backtest_nrmse': 1.0, 'plot_path': 'N/A'}
    
    # ì„±ëŠ¥ ê³„ì‚° (F1-Score, NRMSE)
    y_true, y_pred, y_last = np.array(actuals), np.array(predictions), np.array(last_closes)
    true_dir, pred_dir = (y_true > y_last).astype(int), (y_pred > y_last).astype(int)
    f1 = f1_score(true_dir, pred_dir, zero_division=0)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    nrmse = rmse / (y_true.max() - y_true.min()) if (y_true.max() - y_true.min()) > 1e-5 else 0

    # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ì‹œê°í™” ë¶€ë¶„ ìˆ˜ì • â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
    try:
        # --- ì°¨íŠ¸ ë°ì´í„° í™•ì¥ ---
        # 1. xì¶•(ë‚ ì§œ)ì„ í•˜ë£¨ ë” í™•ì¥
        last_backtest_date = backtest_dates.iloc[-1]
        next_pred_date = get_next_trading_day(last_backtest_date, all_trading_dates)
        plot_dates = backtest_dates[-len(y_pred):].tolist() + [next_pred_date]
        
        # 2. yì¶•(ì˜ˆì¸¡ê°€)ì— 'ë‚´ì¼ ì˜ˆì¸¡ê°€' ì¶”ê°€
        plot_predictions = y_pred.tolist() + [next_day_prediction]

        # --- ì‹œê°í™” ---
        plt.style.use('seaborn-v0_8-darkgrid')
        # ... (í°íŠ¸ ì„¤ì • ë¶€ë¶„ì€ ì´ì „ ë‹µë³€ê³¼ ë™ì¼) ...
        plt.rc('font', family='Malgun Gothic' if sys.platform.startswith('win') else 'AppleGothic')
        plt.rcParams['axes.unicode_minus'] = False

        fig, ax = plt.subplots(figsize=(12, 6))
        # ì‹¤ì œ ì£¼ê°€ëŠ” ê¸°ì¡´ëŒ€ë¡œ í‘œì‹œ
        ax.plot(backtest_dates[-len(y_true):], y_true, label='ì‹¤ì œ ì£¼ê°€ (Actual Price)', marker='o', markersize=4, linestyle='-')
        # ì˜ˆì¸¡ ì£¼ê°€ëŠ” í™•ì¥ëœ ë°ì´í„°ë¡œ í‘œì‹œ
        ax.plot(plot_dates, plot_predictions, label='ì˜ˆì¸¡ ì£¼ê°€ (Predicted Price)', marker='x', markersize=4, linestyle='--')
        
        ax.set_title(f'ìµœê·¼ {backtest_days}ì¼ ë°±í…ŒìŠ¤íŠ¸: {ticker_name} ({ticker})', fontsize=16)
        ax.set_ylabel('ì¢…ê°€ (KRW)', fontsize=12)
        ax.legend(fontsize=12)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plot_path = cfg.BACKTEST_PLOTS_DIR / f"{ticker}_backtest.png"
        plt.savefig(plot_path)
        plt.close(fig)

    except Exception as e:
        print(f"Plotting Error for {ticker}: {e}")
        plot_path = "N/A"
    # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² ì—¬ê¸°ê¹Œì§€ ìˆ˜ì • â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²

    return {'backtest_f1': round(f1, 4), 'backtest_nrmse': round(nrmse, 4), 'plot_path': str(plot_path)}
'''    
    

def predict_worker(args: Tuple) -> List[Dict]:
    #ticker_info, dates_to_predict, all_trading_dates = args
    #cfg = Config()
    # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ì´ ë¶€ë¶„ ìˆ˜ì • â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
    # ì¸ìë¡œ ë°›ì€ cfgë¥¼ ì‚¬ìš©
    ticker_info, dates_to_predict, all_trading_dates, cfg = args
    # cfg = Config() # <-- ì´ ì¤„ì„ ë°˜ë“œì‹œ ì‚­ì œ!
    # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² ì—¬ê¸°ê¹Œì§€ ìˆ˜ì • â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
    
    ticker = ticker_info['ticker']
    
    results = []
    
    try:
        data_handler = DataHandler(cfg.PROCESSED_DB_PATH, cfg.SOURCE_DB_PATH, cfg.DB_TIMEOUT)
        model_seq_len = MODEL_CONFIGS[ticker_info['config_name']].seq_len
        
        for base_date in dates_to_predict:
            base_date_str = base_date.strftime('%Y-%m-%d')
            
            df_recent = data_handler.get_price_data_until(ticker, base_date_str, model_seq_len)
            
            if df_recent is None or len(df_recent) < model_seq_len:
                results.append({'ticker': ticker, 'base_date': base_date_str, 'status': 'Failure', 'reason': 'Insufficient data'})
                continue

            last_known_date = df_recent['date'].iloc[-1]
            next_trading_day = get_next_trading_day(last_known_date, all_trading_dates)
            
            # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ë””ë²„ê¹… ì½”ë“œ ì¶”ê°€ â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
            # íŠ¹ì • ì¢…ëª©ê³¼ ë‚ ì§œì— ëŒ€í•´ì„œë§Œ ì¶œë ¥í•˜ì—¬ ë¡œê·¸ê°€ ë„ˆë¬´ ë§ì•„ì§€ëŠ” ê²ƒì„ ë°©ì§€
            if ticker == '459580' and '2025-08-05' in base_date_str:
                print(f"\n[DEBUG] Ticker: {ticker}")
                print(f"  - í˜„ì¬ ë£¨í”„ì˜ base_date: {base_date.date()}")
                print(f"  - DBì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ë‚ ì§œ (last_known_date): {last_known_date.date()}")
                print(f"  - ê³„ì‚°ëœ ë‹¤ìŒ ì˜ì—…ì¼ (next_trading_day): {next_trading_day.date()}\n")
            # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² ì—¬ê¸°ê¹Œì§€ ì¶”ê°€ â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
            
            predicted_price = run_inference(ticker, ticker_info, df_recent, next_trading_day, cfg)
            
            if predicted_price is None:
                results.append({'ticker': ticker, 'base_date': base_date_str, 'status': 'Failure', 'reason': 'Inference failed'})
                continue
                
            with open(cfg.METADATA_DIR / f"{ticker}.json") as f: features = json.load(f)['features']
            close_idx = features.index('close')
            last_close_log = df_recent.iloc[-1][features].iloc[close_idx]
            last_close_price = np.expm1(last_close_log)
            
            results.append({
                'ticker': ticker,
                'base_date': base_date_str,
                'prediction_date': next_trading_day.strftime('%Y-%m-%d'),
                'last_close': last_close_price,
                'predicted_close': predicted_price,
                'expected_return': ((predicted_price / last_close_price) - 1) * 100 if last_close_price > 0 else 0,
                'status': 'Success'
            })
            
        return results
    except Exception as e:
        return [{'ticker': ticker, 'status': 'Failure', 'reason': f'Worker Error: {str(e)}'}]
    finally:
        if 'cuda' in str(cfg.device): gc.collect(); torch.cuda.empty_cache()

# ==================================================
# 6. ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ==================================================
def find_best_models_across_versions(cfg: Config, stock_meta: Dict) -> List[Dict]:
    print("\n" + "="*20 + " STEP 1: v13 & v14 ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í†µí•© ì„ ì • " + "="*20)
    
    log_files = {'v13': cfg.V13_RESULT_DIR / "performance_log.csv", 'v14': cfg.V14_RESULT_DIR / "performance_log.csv"}
    all_logs = []
    for version, path in log_files.items():
        if path.exists():
            df = pd.read_csv(path)
            df['ticker'] = df['ticker'].astype(str) # <--- ì´ ì¤„ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.
            df['version'] = version
            all_logs.append(df)
            print(f"  âœ“ {version} ë¡œê·¸ ë¡œë“œ ì™„ë£Œ ({len(df)}ê°œ ê¸°ë¡)")
        else:
            print(f"  - {version} ë¡œê·¸ íŒŒì¼ ì—†ìŒ. ê±´ë„ˆëœë‹ˆë‹¤.")

    if not all_logs: sys.exit("âŒ ë¶„ì„í•  ì„±ëŠ¥ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
    combined_df = pd.concat(all_logs, ignore_index=True)
    stock_df = combined_df[combined_df['model_type'] == 'stock'].copy()
    
    stock_df_sorted = stock_df.sort_values(by=['f1', 'nrmse'], ascending=[False, True])
    best_models_df = stock_df_sorted.drop_duplicates('ticker', keep='first')
    
    targets = []
    for _, row in best_models_df.iterrows():
        version = row['version']
        models_dir = cfg.V13_RESULT_DIR / "models" if version == 'v13' else cfg.V14_RESULT_DIR / "models"
        ticker_meta = stock_meta.get(row['ticker'], {})
        
        targets.append({
            'ticker': row['ticker'],
            'name': ticker_meta.get('name', 'N/A'),
            'market': ticker_meta.get('market', 'N/A'),
            'sector': ticker_meta.get('sector', 'Unknown'),
            'config_name': row['config_name'],
            'f1': row['f1'],
            'nrmse': row['nrmse'],
            'version': version,
            'model_path': models_dir / f"model_{row['ticker']}_{row['config_name']}.pth"
        })
        
    print(f"âœ… ì´ {len(targets)}ê°œ ì¢…ëª©ì— ëŒ€í•œ ìµœì¢… ë² ìŠ¤íŠ¸ ëª¨ë¸ ì„ ì •ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
    return targets

def predict_market_indices(cfg: Config, all_trading_dates: pd.Series) -> Tuple[str, pd.DataFrame, float]:
    print("\n" + "="*20 + " STEP 2: ì‹œì¥ ì§€ìˆ˜ ì˜ˆì¸¡ ë° ë°©í–¥ì„± íŒë‹¨ " + "="*20)
    index_predictions = []
    for ticker in ['KOSPI', 'KOSDAQ']:
        model_info = {'config_name': 'base', 'model_path': cfg.V13_RESULT_DIR / "models" / f"model_{ticker}_base.pth"}
        data_handler = DataHandler(cfg.PROCESSED_DB_PATH, cfg.SOURCE_DB_PATH, cfg.DB_TIMEOUT)
        # [ìˆ˜ì •] .iloc[-1] ì‚¬ìš©
        df_recent = data_handler.get_price_data_until(ticker, all_trading_dates.iloc[-1].strftime('%Y-%m-%d'), MODEL_CONFIGS['base'].seq_len)
        if df_recent is None:
            print(f"  - {ticker} ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ì˜ˆì¸¡ ì‹¤íŒ¨."); continue
        last_known_date = df_recent['date'].iloc[-1]
        next_trading_day = get_next_trading_day(last_known_date, all_trading_dates)
        predicted_price = run_inference(ticker, model_info, df_recent, next_trading_day, cfg)
        if predicted_price:
            with open(cfg.METADATA_DIR / f"{ticker}.json") as f: features = json.load(f)['features']
            close_idx = features.index('close')
            last_close_log = df_recent.iloc[-1][features].iloc[close_idx]
            last_close_price = np.expm1(last_close_log)
            expected_return = ((predicted_price / last_close_price) - 1) * 100
            index_predictions.append({'ticker': ticker, 'predicted_return': expected_return})
    if not index_predictions:
        print("  - ì‹œì¥ ì§€ìˆ˜ ì˜ˆì¸¡ ì‹¤íŒ¨. 'ë³´í•©(Neutral)'ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.")
        return 'Neutral', pd.DataFrame(), 0.0
    df_index = pd.DataFrame(index_predictions)
    avg_return = df_index['predicted_return'].mean()
    market_sentiment = 'Neutral'
    if avg_return >= cfg.MARKET_UP_THRESHOLD: market_sentiment = 'Bullish'
    elif avg_return <= cfg.MARKET_DOWN_THRESHOLD: market_sentiment = 'Bearish'
    print(f"  - í‰ê·  ì˜ˆìƒ ìˆ˜ìµë¥ : {avg_return:.2f}%"); print(f"  - ì‹œì¥ ë°©í–¥ì„± íŒë‹¨: {market_sentiment}")
    return market_sentiment, df_index, avg_return

def diversify_by_sector(df: pd.DataFrame, max_per_sector: int) -> pd.DataFrame:
    if 'sector' not in df.columns or df['sector'].nunique() <= 1:
        return df
    
    diversified_list = []
    sector_count = {}
    df_copy = df.copy()
    for index, row in df_copy.iterrows():
        sector = row.get('sector', 'Unknown')
        if sector_count.get(sector, 0) < max_per_sector:
            diversified_list.append(row)
            sector_count[sector] = sector_count.get(sector, 0) + 1
    return pd.DataFrame(diversified_list)
    
def get_prediction_for_date(ticker: str, model_info: Dict, target_date: datetime, all_trading_dates: pd.Series, cfg: Config) -> Optional[float]:
    """
    íŠ¹ì • 'target_date'ì˜ ì¢…ê°€ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    (ë‚´ë¶€ì ìœ¼ë¡œ target_dateì˜ 'ì´ì „ ê±°ë˜ì¼'ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤)
    """
    data_handler = DataHandler(cfg.PROCESSED_DB_PATH, cfg.SOURCE_DB_PATH, cfg.DB_TIMEOUT)
    model_seq_len = MODEL_CONFIGS[model_info['config_name']].seq_len
    
    # target_dateì˜ ì´ì „ ê±°ë˜ì¼ ì°¾ê¸°
    # all_trading_datesëŠ” ì •ë ¬ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
    try:
        # np.whereë¥¼ ì‚¬ìš©í•˜ì—¬ target_dateì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ê³  1ì„ ë¹¼ì„œ ì´ì „ ì¸ë±ìŠ¤ë¥¼ êµ¬í•©ë‹ˆë‹¤.
        prev_day_index = np.where(all_trading_dates.to_numpy() == np.datetime64(target_date))[0][0] - 1
        if prev_day_index < 0:
            return None
        prev_trading_day = all_trading_dates.iloc[prev_day_index]
    except IndexError:
        # target_dateê°€ ë¦¬ìŠ¤íŠ¸ì— ì—†ëŠ” ê²½ìš° ë“± ì˜ˆì™¸ ì²˜ë¦¬
        return None
    
    # ì´ì „ ê±°ë˜ì¼ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
    df_recent = data_handler.get_price_data_until(ticker, prev_trading_day.strftime('%Y-%m-%d'), model_seq_len)
    if df_recent is None or len(df_recent) < model_seq_len:
        return None
        
    # ì˜ˆì¸¡ ì‹¤í–‰ (ì˜ˆì¸¡ ëª©í‘œì¼ì€ target_dateê°€ ë©ë‹ˆë‹¤)
    predicted_price = run_inference(ticker, model_info, df_recent, target_date, cfg)
    return predicted_price

def main():
    start_time = datetime.now()
    print(f"ğŸš€ FEDformer PPO ë°ì´í„° ìƒì„± ë° ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    cfg = Config()
    print(f"â–¶ï¸ Using device: {cfg.device}")
    
    data_handler = DataHandler(cfg.PROCESSED_DB_PATH, cfg.SOURCE_DB_PATH, cfg.DB_TIMEOUT)
    stock_metadata = data_handler.get_stock_metadata()
    all_trading_dates = data_handler.get_all_trading_dates()

    if all_trading_dates.empty:
        sys.exit("âŒ 'processed_daily_prices' í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

    print("\n" + "="*20 + " ì˜ˆì¸¡ ê¸°ê°„ ì„¤ì • " + "="*20)
    # [ìˆ˜ì •] .iloc[-1] ì‚¬ìš©
    latest_date_str = all_trading_dates.iloc[-1].strftime('%Y-%m-%d')
    start_date_str = input(f"â–¶ï¸ ì˜ˆì¸¡ ì‹œì‘ì¼(YYYY-MM-DD) ì…ë ¥ (ë¯¸ì…ë ¥ ì‹œ ìµœê·¼ 1ì¼): ") or latest_date_str
    end_date_str = input(f"â–¶ï¸ ì˜ˆì¸¡ ì¢…ë£Œì¼(YYYY-MM-DD) ì…ë ¥ (ë¯¸ì…ë ¥ ì‹œ ìµœì‹ ì¼): ") or latest_date_str
    
    dates_to_predict = all_trading_dates[(all_trading_dates >= start_date_str) & (all_trading_dates <= end_date_str)]
    if dates_to_predict.empty:
        sys.exit(f"âŒ í•´ë‹¹ ê¸°ê°„ì— ê±°ë˜ì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
    # [ìˆ˜ì •] .iloc[0], .iloc[-1] ì‚¬ìš©
    print(f"-> ì˜ˆì¸¡ ëŒ€ìƒ ê¸°ê°„: {dates_to_predict.iloc[0].date()} ~ {dates_to_predict.iloc[-1].date()} ({len(dates_to_predict)} ê±°ë˜ì¼)")

    target_models = find_best_models_across_versions(cfg, stock_metadata)
    
    print("\n" + "="*20 + " STEP 2: ê¸°ê°„ ì˜ˆì¸¡ ë³‘ë ¬ ì‹¤í–‰ " + "="*20)
    
    # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ì´ ë¶€ë¶„ ìˆ˜ì • â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
    # ì¸ìì— cfgë¥¼ ì¶”ê°€
    worker_args = [(info, dates_to_predict, all_trading_dates, cfg) for info in target_models]
    # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² ì—¬ê¸°ê¹Œì§€ ìˆ˜ì • â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
    
    #worker_args = [(info, dates_to_predict, all_trading_dates) for info in target_models]

    all_results_list = []
    with mp.Pool(processes=cfg.MAX_CONCURRENT_PROCESSES) as pool:
        for res_list in tqdm(pool.imap_unordered(predict_worker, worker_args), total=len(worker_args), desc="   ì¢…ëª©ë³„ ê¸°ê°„ ì˜ˆì¸¡"):
            if res_list: all_results_list.extend(res_list)
            
    print("\nâœ… ëª¨ë“  ê¸°ê°„ ì˜ˆì¸¡ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    if not all_results_list:
        print("â„¹ï¸ ìœ íš¨í•œ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."); return
        
    print("\n" + "="*20 + " STEP 3: PPOìš© DB ì €ì¥ ë° ì¶”ì²œì£¼ ì„ ì • " + "="*20)
    df_results = pd.DataFrame(all_results_list)
    
    try:
        with sqlite3.connect(cfg.PPO_DB_PATH) as conn:
            df_results.to_sql('predictions', conn, if_exists='append', index=False)
            conn.execute("CREATE INDEX IF NOT EXISTS idx_ticker_date ON predictions (ticker, base_date)")
            conn.execute("""
                DELETE FROM predictions
                WHERE rowid NOT IN (
                    SELECT MIN(rowid)
                    FROM predictions
                    GROUP BY ticker, base_date
                )
            """)
        print(f"  âœ“ ì´ {len(df_results)}ê±´ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ PPOìš© DB '{cfg.PPO_DB_PATH.name}'ì— ëˆ„ì  ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"  âœ— PPOìš© DB ì €ì¥ ì‹¤íŒ¨: {e}")

    # --- ìµœì¢…ì¼ ê¸°ì¤€ ì¶”ì²œ ë¡œì§ ---
    df_last_day_preds = df_results[df_results['base_date'] == end_date_str].copy()
    if df_last_day_preds.empty:
        print("  - ìµœì¢…ì¼ì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ì–´ ì¶”ì²œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
        
    market_sentiment, df_index_pred, avg_market_return = predict_market_indices(cfg, all_trading_dates)
    if market_sentiment == 'Bearish':
        print("\n" + "!"*50); print("âš ï¸  ê²½ê³ : ì‹œì¥ í•˜ë½ì´ ì˜ˆìƒë©ë‹ˆë‹¤. ë³´ìˆ˜ì ì¸ íˆ¬ìë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."); print("!"*50)
    
    df_success = df_last_day_preds[df_last_day_preds['status'] == 'Success'].copy()
    
    # [ìˆ˜ì •] target_modelsë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ join ì¤€ë¹„
    df_meta = pd.DataFrame(target_models)
    # ì¤‘ë³µ ì»¬ëŸ¼(name, market, sector)ì´ ìˆë‹¤ë©´ ë¨¼ì € ì œê±°
    df_success = df_success.drop(columns=['name', 'market', 'sector'], errors='ignore')
    df_success = pd.merge(df_success, df_meta, on='ticker', how='left')

    df_success['avg_volume_20d'] = df_success['ticker'].apply(lambda t: DataHandler(cfg.PROCESSED_DB_PATH, cfg.SOURCE_DB_PATH, cfg.DB_TIMEOUT).get_avg_volume(t))
    df_filtered = df_success[df_success['avg_volume_20d'] > cfg.MIN_AVG_VOLUME].copy()
    
    #NRMSEì™€ F1 Score ê¸°ì¤€ìœ¼ë¡œ ì¶”ê°€ í•„í„°ë§
    nrmse_threshold = 0.25 # NRMSE í—ˆìš© ì„ê³„ê°’ (ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ì¡°ì ˆ)
    f1_threshold = 0.52    # F1 Score ìµœì†Œ ì„ê³„ê°’ (ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ì¡°ì ˆ)
    
    print(f"\n-> í•„í„°ë§ ì¡°ê±´: NRMSE < {nrmse_threshold}, F1 Score >= {f1_threshold}")
    df_filtered = df_filtered[(df_filtered['nrmse'] < nrmse_threshold) & (df_filtered['f1'] >= f1_threshold)]
    print(f"-> NRMSE/F1 í•„í„°ë§ í›„ ë‚¨ì€ ì¢…ëª© ìˆ˜: {len(df_filtered)}ê°œ")
    
    # --- ì˜ˆì¸¡ ì¶”ì„¸ ê¸°ë°˜ ìˆ˜ìµë¥  ê³„ì‚° (ê°€ì¥ ì¤‘ìš”í•œ ë¡œì§) ---
    print("\n-> ì˜ˆì¸¡ ì¶”ì„¸ ê¸°ë°˜ ìˆ˜ìµë¥  ê³„ì‚° ì¤‘...")
    pred_trend_returns = []
    # ìµœì¢… ì˜ˆì¸¡ ê¸°ì¤€ì¼(ì˜¤ëŠ˜)ì„ datetime ê°ì²´ë¡œ ë³€í™˜
    end_date_dt = pd.to_datetime(end_date_str)

    for _, row in tqdm(df_filtered.iterrows(), total=len(df_filtered), desc="   ì˜ˆì¸¡ ì¶”ì„¸ ê³„ì‚°"):
        # Pred_T+1 (ë‚´ì¼ ì˜ˆì¸¡ê°€)ëŠ” ì´ë¯¸ ê³„ì‚°ë˜ì–´ ìˆìŒ
        pred_t_plus_1 = row['predicted_close']
        
        # Pred_T (ì˜¤ëŠ˜ ì˜ˆì¸¡ê°€)ë¥¼ ìƒˆë¡œ ê³„ì‚°
        model_info = {'config_name': row['config_name'], 'model_path': row['model_path']}
        pred_t = get_prediction_for_date(row['ticker'], model_info, end_date_dt, all_trading_dates, cfg)
        
        if pred_t is not None and pred_t > 0:
            # (ë‚´ì¼ ì˜ˆì¸¡ê°€ / ì˜¤ëŠ˜ ì˜ˆì¸¡ê°€ - 1) * 100
            trend_return = ((pred_t_plus_1 / pred_t) - 1) * 100
            pred_trend_returns.append(trend_return)
        else:
            # ê³„ì‚° ë¶ˆê°€ì‹œ 0%ë¡œ ì²˜ë¦¬í•˜ì—¬ ì¶”ì²œì—ì„œ ì œì™¸
            pred_trend_returns.append(0)

    # ê³„ì‚°ëœ ì˜ˆì¸¡ ì¶”ì„¸ ìˆ˜ìµë¥ ì„ ìƒˆ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
    df_filtered['pred_trend_return'] = pred_trend_returns

    # --- ìƒˆë¡œìš´ ìˆ˜ìµë¥  ê¸°ë°˜ìœ¼ë¡œ í•„í„°ë§ ë° ì •ë ¬ ---
    print("\n-> ìƒìŠ¹ ì¶”ì„¸ ì˜ˆì¸¡ ì¢…ëª© í•„í„°ë§...")
    # "ìƒìŠ¹"ì´ ì˜ˆìƒë˜ëŠ” ì¢…ëª©ë§Œ í•„í„°ë§ (pred_trend_return > 0)
    df_filtered = df_filtered[df_filtered['pred_trend_return'] > 0]
    print(f"-> ìƒìŠ¹ ì¶”ì„¸ í•„í„°ë§ í›„ ë‚¨ì€ ì¢…ëª© ìˆ˜: {len(df_filtered)}ê°œ")

    # ì˜ˆì¸¡ëœ ìƒìŠ¹ë¥ ì´ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    df_filtered = df_filtered.sort_values('pred_trend_return', ascending=False)
    
    # --- ì‹œì¥ ìƒí™©ì— ë”°ë¼ ìµœì¢… ì¶”ì²œ ê°œìˆ˜ ê²°ì • ---
    if market_sentiment == 'Bullish': num_recommend = cfg.RECOMMEND_COUNT_BULL
    elif market_sentiment == 'Bearish': num_recommend = cfg.RECOMMEND_COUNT_BEAR
    else: num_recommend = cfg.RECOMMEND_COUNT_NEUTRAL
        
    df_top_n = df_filtered.head(num_recommend * 2)
    df_recommended = diversify_by_sector(df_top_n, max_per_sector=cfg.MAX_PER_SECTOR).copy()
    df_recommended = df_recommended.head(num_recommend)
    
    if not df_recommended.empty:
        print("\n" + "="*20 + " STEP 4: ì¶”ì²œì£¼ ë°±í…ŒìŠ¤íŒ… ë° ì‹œê°í™” " + "="*20)
        
        # [í•µì‹¬ ì¶”ê°€] ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰ ë° ê²°ê³¼ ë³‘í•©
        backtest_results = []
        for _, row in tqdm(df_recommended.iterrows(), total=len(df_recommended), desc="   ì¶”ì²œì£¼ ë°±í…ŒìŠ¤íŒ…"):
            model_info = {'config_name': row['config_name'], 'model_path': row['model_path']}
            
            #result = run_backtest_and_visualize(row['ticker'], row['name'], model_info, all_trading_dates, cfg, backtest_days=20)
            
            # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ì´ ë¶€ë¶„ ìˆ˜ì • â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
            # 'ë‚´ì¼ ì˜ˆì¸¡ê°€'ì¸ row['predicted_close']ë¥¼ ì¶”ê°€ë¡œ ì „ë‹¬
            result = run_backtest_and_visualize(
                row['ticker'], 
                row['name'], 
                model_info, 
                all_trading_dates, 
                cfg, 
                row['predicted_close'], # 6ë²ˆì§¸ ì¸ì (next_day_predictionìœ¼ë¡œ ì „ë‹¬ë¨)
                backtest_days=20      # 7ë²ˆì§¸ ì¸ì (í‚¤ì›Œë“œë¡œ ì „ë‹¬ë¨)
            )
            # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² ì—¬ê¸°ê¹Œì§€ ìˆ˜ì • â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
            
            result['ticker'] = row['ticker']
            backtest_results.append(result)

        if backtest_results:
            df_backtest = pd.DataFrame(backtest_results)
            df_recommended = pd.merge(df_recommended, df_backtest, on='ticker', how='left')

        df_recommended.loc[:, 'confidence'] = df_recommended['f1'].apply(lambda x: 'â­â­â­' if x > 0.6 else 'â­â­' if x > 0.55 else 'â­')
    
    print(f"-> ì‹œì¥ ìƒí™©({market_sentiment}) ë° í•„í„°ë§ í›„ ìµœì¢… {len(df_recommended)}ê°œ ì¢…ëª©ì„ ì¶”ì²œí•©ë‹ˆë‹¤.")

    # ì—‘ì…€ ìš”ì•½ ì‹œíŠ¸ì— ë“¤ì–´ê°ˆ ë°ì´í„° ìƒì„±
    summary_data = {
        'í•­ëª©': ['ì˜ˆì¸¡ ê¸°ì¤€ì¼', 'ì‹œì¥ ë°©í–¥ì„± ì˜ˆì¸¡', 'KOSPI/KOSDAQ í‰ê·  ìˆ˜ìµë¥ (%)', 'ìƒìŠ¹ì¥ ê¸°ì¤€(%)', 'í•˜ë½ì¥ ê¸°ì¤€(%)', 'ìµœì¢… ì¶”ì²œ ì¢…ëª© ìˆ˜'],
        'ë‚´ìš©': [
            end_date_str, 
            market_sentiment, 
            f"{avg_market_return:.2f}", 
            f">= {cfg.MARKET_UP_THRESHOLD}", 
            f"<= {cfg.MARKET_DOWN_THRESHOLD}", 
            len(df_recommended)
        ]
    }
    
    try:
        with pd.ExcelWriter(cfg.RECOMMENDATION_EXCEL_PATH, engine='openpyxl') as writer:
            # [ìˆ˜ì •] ê° DataFrameì´ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ì‹œíŠ¸ë¥¼ ìƒì„±í•˜ë„ë¡ ë³€ê²½
            
            # ì‹œíŠ¸ 1: ìš”ì•½ ì •ë³´
            #if 'summary_data' in locals() and summary_data:
            pd.DataFrame(summary_data).to_excel(writer, sheet_name='Prediction_Summary', index=False)
            
            # ì¶”ì²œ ì¢…ëª© ì‹œíŠ¸ ì €ì¥ ì‹œ ì»¬ëŸ¼ëª… ë³€ê²½
            if not df_recommended.empty:
                rec_cols = [
                    'prediction_date', 'ticker', 'name', 'sector', 
                    'pred_trend_return',  # <--- 'expected_return'ì„ ì´ê²ƒìœ¼ë¡œ ë³€ê²½
                    'confidence', 'f1', 'nrmse', 
                    'backtest_f1', 'backtest_nrmse',
                    'predicted_close', 'last_close', 'avg_volume_20d', 'market', 
                    'version', 'config_name', 'plot_path'
                ]
                cols_to_save = [c for c in rec_cols if c in df_recommended.columns]
                df_recommended[cols_to_save].to_excel(writer, sheet_name='Top_Recommendations', index=False)

            # ì‹œíŠ¸ 3: ìµœì¢…ì¼ ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼
            if not df_last_day_preds.empty:
                df_last_day_preds.to_excel(writer, sheet_name='Last_Day_All_Results', index=False)
            
            # ì‹œíŠ¸ 4: ì‹œì¥ ì§€ìˆ˜ ì˜ˆì¸¡ ê²°ê³¼
            if not df_index_pred.empty:
                df_index_pred.to_excel(writer, sheet_name='Market_Index_Prediction', index=False)
                
        print(f"ğŸ“„ ìµœì¢… ì¶”ì²œ ë¦¬í¬íŠ¸ë¥¼ '{cfg.RECOMMENDATION_EXCEL_PATH}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì—‘ì…€ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        
    
    end_time = datetime.now()
    print(f"\n\nğŸ ëª¨ë“  ì‘ì—… ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {end_time - start_time})")

if __name__ == '__main__':
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    main()
