# -*- coding: utf-8 -*-
"""
FEDformer PPO ë°ì´í„° ìƒì„± ë° ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ (v22.1 - ë²„ê·¸ ìˆ˜ì •)

ì£¼ìš” ê¸°ëŠ¥ (v22 ëŒ€ë¹„ ê°œì„ ):
1.  [ë²„ê·¸ ìˆ˜ì •] pandas Seriesì—ì„œ ë§ˆì§€ë§‰ ë‚ ì§œë¥¼ ê°€ì ¸ì˜¬ ë•Œ ë°œìƒí•˜ë˜ KeyErrorë¥¼ .iloc[-1]ì„ ì‚¬ìš©í•˜ì—¬ í•´ê²°.
2.  [ì•ˆì •ì„± ê°•í™”] í•„í„°ë§ëœ ë‚ ì§œ Seriesì—ì„œë„ .ilocë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì ì¸ ì¸ë±ì‹± ë³´ì¥.
"""
import os
import sys
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import sqlite3
import gc
import json
import joblib
import copy
from pathlib import Path
from types import SimpleNamespace
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, mean_squared_error
from tqdm import tqdm
from datetime import datetime, timedelta
import multiprocessing as mp
from typing import List, Dict, Any, Tuple, Optional
from contextlib import contextmanager
import matplotlib
from matplotlib import font_manager as fm # <--- font_manager import
matplotlib.use("Agg")
import matplotlib.pyplot as plt




# ==================================================
# 1. ê²½ë¡œ ë° ëª¨ë“ˆ ì„¤ì •
# ==================================================
try:
    PROJECT_ROOT = Path(__file__).resolve().parent
except NameError:
    PROJECT_ROOT = Path(os.getcwd())

FEDFORMER_ROOT = PROJECT_ROOT / 'FEDformer-master'
if str(PROJECT_ROOT) not in sys.path: sys.path.insert(0, str(PROJECT_ROOT))
if str(FEDFORMER_ROOT) not in sys.path: sys.path.insert(0, str(FEDFORMER_ROOT))
try:
    from FEDformer import Model as FEDformer_base
    from utils.timefeatures import time_features
except ImportError as e:
    sys.exit(f"âŒ CRITICAL ERROR: FEDformer ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

# ==================================================
# 2. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
# ==================================================
class FEDformerWithEmbedding(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = copy.deepcopy(config)
        self.ticker_embedding = nn.Embedding(self.config.num_classes, self.config.embedding_dim)
        self.embedding_dropout = nn.Dropout(self.config.embedding_dropout)
        config_for_base = copy.deepcopy(self.config)
        config_for_base.enc_in += self.config.embedding_dim
        config_for_base.dec_in += self.config.embedding_dim
        self.base_model = FEDformer_base(config_for_base)
    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, ticker_ids):
        emb = self.ticker_embedding(ticker_ids.squeeze(-1))
        emb = self.embedding_dropout(emb)
        emb_enc = emb.unsqueeze(1).repeat(1, self.config.seq_len, 1)
        emb_dec = emb.unsqueeze(1).repeat(1, self.config.label_len + self.config.pred_len, 1)
        x_enc_with_emb = torch.cat([x_enc, emb_enc], dim=-1)
        x_dec_with_emb = torch.cat([x_dec, emb_dec], dim=-1)
        return self.base_model(x_enc_with_emb, x_mark_enc, x_dec_with_emb, x_mark_dec)

# ==================================================
# 3. ì„¤ì • í´ë˜ìŠ¤ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ==================================================
class Config:
    def __init__(self):
        # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ í”„ë¡œì íŠ¸ ë£¨íŠ¸
        self.PROJECT_ROOT = PROJECT_ROOT
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ (í˜„ì¬ í”„ë¡œì íŠ¸ ë‚´)
        self.V13_RESULT_DIR = self.PROJECT_ROOT / "training_results_v13"
        self.V14_RESULT_DIR = self.PROJECT_ROOT / "training_results_v14_retrain"
        self.PROCESSED_DATA_DIR = self.PROJECT_ROOT / "processed_data_v9"
        
        self.PROCESSED_DB_PATH = self.PROCESSED_DATA_DIR / "processed_stock_data.db"
        self.METADATA_DIR = self.PROCESSED_DATA_DIR / "metadata"
        self.SCALER_DIR = self.PROCESSED_DATA_DIR / "scalers"
        
        # ì‹¤í–‰ ì‹œì  ê¸°ì¤€ì˜ ê³ ìœ í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # PPO ì˜ˆì¸¡ ê²°ê³¼ ìƒìœ„ í´ë”
        predictions_base_dir = self.PROJECT_ROOT / "predictions_v22_ppo"
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì´ë¦„ìœ¼ë¡œ í•˜ëŠ” ì‹¤í–‰ ê²°ê³¼ í´ë” ìƒì„±
        self.RUN_OUTPUT_DIR = predictions_base_dir / run_timestamp
        self.RUN_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        
        # ê° ê²°ê³¼ë¬¼ì˜ ê²½ë¡œ
        self.BACKTEST_PLOTS_DIR = self.RUN_OUTPUT_DIR / "backtest_plots"
        self.BACKTEST_PLOTS_DIR.mkdir(exist_ok=True)
        
        self.RECOMMENDATION_EXCEL_PATH = self.RUN_OUTPUT_DIR / "final_recommendations.xlsx"
        self.PPO_DB_PATH = predictions_base_dir / "ppo_predictions.db"
        
        # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ì´ ë¶€ë¶„ ì „ì²´ ìˆ˜ì • â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
        # SOURCE_DB_PATH: ì›ë³¸ ì£¼ê°€ ë°ì´í„° (stock_data.db)
        # í˜„ì¬ í”„ë¡œì íŠ¸ í´ë” ë‚´ì—ì„œ ì°¾ê³ , ì—†ìœ¼ë©´ ë¶€ëª¨ í´ë”ì—ì„œ ì°¾ê¸°
        self.SOURCE_DB_PATH = self.PROJECT_ROOT / "stock_data.db"
        
        if not self.SOURCE_DB_PATH.exists():
            # í˜„ì¬ í´ë”ì— ì—†ìœ¼ë©´ ë¶€ëª¨ í´ë”ì˜ ë‹¤ë¥¸ í”„ë¡œì íŠ¸ë“¤ ê²€ìƒ‰
            parent_dir = self.PROJECT_ROOT.parent
            
            # ê°€ëŠ¥í•œ í”„ë¡œì íŠ¸ í´ë”ëª… ë¦¬ìŠ¤íŠ¸ (ìš°ì„ ìˆœìœ„ ìˆœ)
            possible_folders = [
                "DTB_project",
                "DeepTrader_baekdoosan", 
                "stock_analysis"
            ]
            
            for folder_name in possible_folders:
                candidate_path = parent_dir / folder_name / "stock_data.db"
                if candidate_path.exists():
                    self.SOURCE_DB_PATH = candidate_path
                    print(f"âœ“ stock_data.db ë°œê²¬: {candidate_path}")
                    break
            
            # ê·¸ë˜ë„ ëª» ì°¾ìœ¼ë©´ í˜„ì¬ í´ë” ê¸°ì¤€ìœ¼ë¡œ ì„¤ì • (ì—ëŸ¬ëŠ” ë‚˜ì¤‘ì— ë°œìƒ)
            if not self.SOURCE_DB_PATH.exists():
                print(f"âš ï¸  ê²½ê³ : stock_data.dbë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"   í˜„ì¬ ê²½ë¡œë¡œ ì„¤ì •: {self.SOURCE_DB_PATH}")
                print(f"   ì‹¤í–‰ ì „ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² ì—¬ê¸°ê¹Œì§€ ìˆ˜ì • â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
        
        self.MAX_CONCURRENT_PROCESSES = max(1, mp.cpu_count() // 2)
        self.DB_TIMEOUT = 30
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        self.RECOMMEND_COUNT_BULL = 7
        self.RECOMMEND_COUNT_NEUTRAL = 5
        self.RECOMMEND_COUNT_BEAR = 3
        self.MARKET_UP_THRESHOLD = 0.2
        self.MARKET_DOWN_THRESHOLD = -0.3
        
        self.MIN_AVG_VOLUME = 100000
        self.MAX_PER_SECTOR = 2

MODEL_CONFIGS = {
    'base': SimpleNamespace(config_name='base', version='Fourier', model_name='FEDformer', seq_len=60, label_len=30, pred_len=1, moving_avg=25, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff_multiplier=4, dropout=0.1, activation='gelu', output_attention=False, modes=64, mode_select='random', L=3, base='Fourier', cross_activation='tanh', embed='timeF', freq='d'),
    'tuning_v1': SimpleNamespace(config_name='tuning_v1', version='Fourier', model_name='FEDformer', seq_len=60, label_len=30, pred_len=1, moving_avg=25, d_model=256, n_heads=8, e_layers=2, d_layers=1, d_ff_multiplier=4, dropout=0.2, activation='gelu', output_attention=False, modes=64, mode_select='random', L=3, base='Fourier', cross_activation='tanh', embed='timeF', freq='d'),
    'shallow_wide': SimpleNamespace(config_name='shallow_wide', version='Fourier', model_name='FEDformer', seq_len=60, label_len=30, pred_len=1, moving_avg=25, d_model=384, n_heads=12, e_layers=1, d_layers=1, d_ff_multiplier=3, dropout=0.35, activation='gelu', output_attention=False, modes=48, mode_select='random', L=3, base='Fourier', cross_activation='tanh', embed='timeF', freq='d'),
    'deep_narrow': SimpleNamespace(config_name='deep_narrow', version='Fourier', model_name='FEDformer', seq_len=60, label_len=30, pred_len=1, moving_avg=25, d_model=256, n_heads=8, e_layers=4, d_layers=2, d_ff_multiplier=4, dropout=0.2, activation='gelu', output_attention=False, modes=64, mode_select='random', L=4, base='Fourier', cross_activation='tanh', embed='timeF', freq='d'),
    'high_freq': SimpleNamespace(config_name='high_freq', version='Fourier', model_name='FEDformer', seq_len=40, label_len=20, pred_len=1, moving_avg=15, d_model=320, n_heads=10, e_layers=2, d_layers=1, d_ff_multiplier=4, dropout=0.25, activation='gelu', output_attention=False, modes=96, mode_select='random', L=3, base='Fourier', cross_activation='tanh', embed='timeF', freq='d'),
    'low_freq': SimpleNamespace(config_name='low_freq', version='Fourier', model_name='FEDformer', seq_len=80, label_len=40, pred_len=1, moving_avg=35, d_model=384, n_heads=8, e_layers=2, d_layers=1, d_ff_multiplier=4, dropout=0.15, activation='gelu', output_attention=False, modes=32, mode_select='low', L=3, base='Fourier', cross_activation='tanh', embed='timeF', freq='d'),
    'regularized': SimpleNamespace(config_name='regularized', version='Fourier', model_name='FEDformer', seq_len=60, label_len=30, pred_len=1, moving_avg=25, d_model=192, n_heads=6, e_layers=2, d_layers=1, d_ff_multiplier=2, dropout=0.4, activation='gelu', output_attention=False, modes=40, mode_select='random', L=3, base='Fourier', cross_activation='tanh', embed='timeF', freq='d')
}
for cfg_item in MODEL_CONFIGS.values():
    cfg_item.d_ff = int(cfg_item.d_model * cfg_item.d_ff_multiplier)
    cfg_item.embedding_dim=16; cfg_item.embedding_dropout=0.2

# ==================================================
# 4. ë°ì´í„° ë° ìœ í‹¸ë¦¬í‹°
# ==================================================
@contextmanager
def suppress_stdout():
    """ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ë°œìƒí•˜ëŠ” print ì¶œë ¥ì„ ìˆ¨ê¹ë‹ˆë‹¤."""
    with open(os.devnull, "w") as devnull:
        old_stdout = sys.stdout
        sys.stdout = devnull
        try:
            yield
        finally:
            sys.stdout = old_stdout

class DataHandler:
    def __init__(self, processed_db_path: Path, source_db_path: Path, timeout: int):
        if not processed_db_path.exists() or not source_db_path.exists():
            sys.exit(f"âŒ DB íŒŒì¼ ì˜¤ë¥˜: DB ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\n- {processed_db_path}\n- {source_db_path}")
        self.processed_db_path = processed_db_path
        self.source_db_path = source_db_path
        self.timeout = timeout
    def get_stock_metadata(self) -> Dict[str, Dict[str, str]]:
        with sqlite3.connect(self.source_db_path, timeout=self.timeout) as conn:
            try:
                df = pd.read_sql_query("SELECT ticker, name, market, sector FROM stocks", conn)
                df['ticker'] = df['ticker'].astype(str) # <--- ì´ ì¤„ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.
            except (pd.errors.DatabaseError, sqlite3.OperationalError) as e:
                if 'no such column: sector' in str(e):
                    print("  - ê²½ê³ : 'stocks' í…Œì´ë¸”ì— 'sector' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì„¹í„° ë¶„ì‚° ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
                    df = pd.read_sql_query("SELECT ticker, name, market FROM stocks", conn)
                else: raise e
        return df.set_index('ticker').to_dict('index')
    def get_all_trading_dates(self) -> pd.Series:
        """[ìˆ˜ì •] ë°˜í™˜ íƒ€ì…ì„ DatetimeIndexì—ì„œ Seriesë¡œ ëª…ì‹œ"""
        with sqlite3.connect(self.processed_db_path, timeout=self.timeout) as conn:
            df = pd.read_sql_query("SELECT DISTINCT date FROM processed_daily_prices ORDER BY date", conn)
        return pd.to_datetime(df['date'])
    # 6prediction_integrated_PPO_prepare.py íŒŒì¼ì˜ DataHandler í´ë˜ìŠ¤

    def get_price_data_until(self, ticker: str, end_date: str, num_rows: int) -> Optional[pd.DataFrame]:
        with sqlite3.connect(self.processed_db_path, timeout=self.timeout) as conn:
            # [ìˆ˜ì •] date ì»¬ëŸ¼ì„ ëª…ì‹œì ìœ¼ë¡œ date íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
            query = f"SELECT * FROM processed_daily_prices WHERE ticker = ? AND date(date) <= ? ORDER BY date DESC LIMIT ?"
            df = pd.read_sql_query(query, conn, params=(ticker, end_date, num_rows))
        if df.empty or len(df) < num_rows: return None
        df['date'] = pd.to_datetime(df['date'])
        return df.sort_values('date', ascending=True).reset_index(drop=True)
        
        
    # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ì´ ë¶€ë¶„ì„ ì¶”ê°€ â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
    def get_avg_volume(self, ticker: str, days: int = 20) -> float:
        """ì§€ì •ëœ ì¢…ëª©ì˜ ìµœê·¼ Nì¼ í‰ê·  ê±°ë˜ëŸ‰ì„ ì›ë³¸ DBì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."""
        # ë³€í™˜ë˜ì§€ ì•Šì€ ì›ë³¸ ê±°ë˜ëŸ‰ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ source_db_pathë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        with sqlite3.connect(self.source_db_path, timeout=self.timeout) as conn:
            try:
                # í•´ë‹¹ ì¢…ëª©ì˜ ë°ì´í„°ë¥¼ ë‚ ì§œ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìµœê·¼ `days`ê°œ ë§Œí¼ ì¡°íšŒ
                query = f"SELECT volume FROM daily_prices WHERE ticker = ? ORDER BY date DESC LIMIT ?"
                df = pd.read_sql_query(query, conn, params=(ticker, days))
                
                # ë°ì´í„°ê°€ ì¡´ì¬í•˜ë©´ ê±°ë˜ëŸ‰ì˜ í‰ê· ì„ ê³„ì‚°í•˜ê³ , ì—†ìœ¼ë©´ 0ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
                if not df.empty:
                    return float(df['volume'].mean())
                else:
                    return 0.0
            except Exception as e:
                # í˜¹ì‹œ ëª¨ë¥¼ ì˜¤ë¥˜ ë°œìƒ ì‹œ ê²½ê³ ë¥¼ ì¶œë ¥í•˜ê³  0ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
                print(f"Warning: Could not get avg volume for {ticker}. Error: {e}")
                return 0.0
                

def get_next_trading_day(last_date: datetime, all_trading_dates: pd.Series) -> datetime:
    future_dates = all_trading_dates[all_trading_dates > last_date]
    return future_dates.iloc[0] if not future_dates.empty else last_date + pd.tseries.offsets.BDay(1)

def inverse_transform_price(scaler: StandardScaler, scaled_value: float, close_idx: int) -> float:
    dummy_array = np.zeros((1, scaler.n_features_in_))
    dummy_array[0, close_idx] = scaled_value
    unscaled_log_price = scaler.inverse_transform(dummy_array)[0, close_idx]
    return np.expm1(unscaled_log_price)

# ==================================================
# 5. ì˜ˆì¸¡ ì‹¤í–‰ ë¡œì§
# ==================================================
def run_inference(ticker: str, model_info: Dict, df_recent: pd.DataFrame, next_trading_day: datetime, cfg: Config) -> Optional[float]:
    try:
        config_name = model_info['config_name']
        model_path = model_info['model_path']
        if not model_path.exists(): 
            return None
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œí•´ì„œ í•™ìŠµëœ feature ê°œìˆ˜ í™•ì¸
        checkpoint = torch.load(model_path, map_location=cfg.device, weights_only=False)
        saved_weight_shape = checkpoint['model_state_dict']['enc_embedding.value_embedding.tokenConv.weight'].shape
        trained_features_count = saved_weight_shape[1]  # 18
        
        with suppress_stdout():
            model_config = copy.deepcopy(MODEL_CONFIGS[config_name])
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì „ì²´ features ê°€ì ¸ì˜¤ê¸°
            with open(cfg.METADATA_DIR / f"{ticker}.json", 'r') as f: 
                metadata = json.load(f)
                all_features = metadata['features']
            
            # í•™ìŠµì‹œì™€ ë™ì¼í•œ ê°œìˆ˜ì˜ featuresë§Œ ì‚¬ìš© (ì²˜ìŒ 18ê°œ)
            features = all_features[:trained_features_count]
            
            # ëª¨ë¸ ì„¤ì • - 18ê°œë¡œ ë§ì¶¤!
            model_config.enc_in = model_config.dec_in = trained_features_count  # 28ì´ ì•„ë‹ˆë¼ 18
            model_config.c_out = 1
            
            # FEDformer_base ëª¨ë¸ ìƒì„±
            model = FEDformer_base(model_config).to(cfg.device)
            model.load_state_dict(checkpoint['model_state_dict'])
            model.eval()

        # Scaler ë¡œë“œ
        scaler = joblib.load(cfg.SCALER_DIR / f"{ticker}.pkl")
        
        # scalerì˜ feature ê°œìˆ˜ë„ í™•ì¸
        if hasattr(scaler, 'n_features_in_'):
            if scaler.n_features_in_ != trained_features_count:
                # Scalerê°€ 28ê°œë¡œ í•™ìŠµëë‹¤ë©´ ì²˜ìŒ 18ê°œë§Œ ì‚¬ìš©
                features = all_features[:trained_features_count]
        
        # ë°ì´í„° ì „ì²˜ë¦¬ - í•™ìŠµì‹œ ì‚¬ìš©ëœ 18ê°œ featuresë§Œ ì„ íƒ
        data_to_scale = df_recent[features].astype(np.float32)
        
        # Scalerê°€ 28ê°œ featureë¥¼ ê¸°ëŒ€í•œë‹¤ë©´ dummy ë°°ì—´ ìƒì„±
        if hasattr(scaler, 'n_features_in_') and scaler.n_features_in_ > trained_features_count:
            
            # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ [WARNING ìˆ˜ì •] â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
            # scalerê°€ feature nameì„ ê°€ì§€ê³  í•™ìŠµë˜ì—ˆëŠ”ì§€ í™•ì¸
            if hasattr(scaler, 'feature_names_in_'):
                # 1. scalerê°€ ê¸°ëŒ€í•˜ëŠ” ì „ì²´ feature name (ì˜ˆ: 28ê°œ)
                scaler_features = scaler.feature_names_in_
                
                # 2. 0ìœ¼ë¡œ ì±„ì›Œì§„ (N, 28) DataFrame ìƒì„± (NumPy ë°°ì—´ ëŒ€ì‹ )
                full_df = pd.DataFrame(0.0, index=data_to_scale.index, columns=scaler_features)
                
                # 3. ì´ DataFrameì— 18ê°œ feature ê°’ ë³µì‚¬
                #    (features ë¦¬ìŠ¤íŠ¸ì˜ ì´ë¦„ì´ scaler_featuresì— í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•¨)
                try:
                    full_df[features] = data_to_scale
                except ValueError:
                    # í˜¹ì‹œ ëª¨ë¥¼ ë¶ˆì¼ì¹˜ ì‹œ, ê°•ì œë¡œ ê°’ë§Œ ë³µì‚¬ (ê²½ê³ ê°€ ë‹¤ì‹œ ë°œìƒí•  ìˆ˜ ìˆìŒ)
                    full_df.iloc[:, :trained_features_count] = data_to_scale.values
                
                # 4. NumPy ë°°ì—´(full_array) ëŒ€ì‹  DataFrame(full_df)ì„ transform
                scaled_data_full = scaler.transform(full_df) # <-- No Warning
                
                # 5. ë³€í™˜ëœ NumPy ë°°ì—´ì—ì„œ 18ê°œ featureë§Œ ë‹¤ì‹œ ìŠ¬ë¼ì´ì‹±
                scaled_data = scaled_data_full[:, :trained_features_count]
            
            else:
                # scalerê°€ feature name ì—†ì´ í•™ìŠµëœ ê²½ìš° (ê²½ê³ ê°€ ê³„ì† ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)
                full_array = np.zeros((len(data_to_scale), scaler.n_features_in_))
                # .values ì‚¬ìš© (ì œì•ˆí•˜ì‹  ë‚´ìš© ì ìš©)
                full_array[:, :trained_features_count] = data_to_scale.values 
                scaled_data = scaler.transform(full_array)[:, :trained_features_count]
            # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² [WARNING ìˆ˜ì • ë] â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
                
        else:
            # scalerì˜ feature ê°œìˆ˜(18)ì™€ ëª¨ë¸ì˜ feature ê°œìˆ˜(18)ê°€ ë™ì¼í•œ ê²½ìš°
            # data_to_scale (DataFrame)ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ ê²½ê³ ê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
            scaled_data = scaler.transform(data_to_scale)
        
        # ì‹œê°„ íŠ¹ì§• ìƒì„±
        time_marks = time_features(pd.DatetimeIndex(df_recent['date']), freq=model_config.freq).transpose()
        
        # í…ì„œ ë³€í™˜
        x_enc = torch.tensor(scaled_data, dtype=torch.float32).unsqueeze(0).to(cfg.device)
        x_mark_enc = torch.tensor(time_marks, dtype=torch.float32).unsqueeze(0).to(cfg.device)
        
        # ë””ì½”ë” ì…ë ¥ ìƒì„±
        dec_inp_context = x_enc[:, -model_config.label_len:, :]
        dec_inp_zeros = torch.zeros(1, model_config.pred_len, model_config.dec_in, device=cfg.device)
        x_dec = torch.cat([dec_inp_context, dec_inp_zeros], dim=1)
        
        # ë””ì½”ë” ì‹œê°„ íŠ¹ì§•
        decoder_dates = pd.DatetimeIndex(df_recent['date'].iloc[-model_config.label_len:].tolist() + [next_trading_day])
        x_mark_dec = torch.tensor(time_features(decoder_dates, freq=model_config.freq).T, dtype=torch.float32).unsqueeze(0).to(cfg.device)
        
        # ì¶”ë¡  ì‹¤í–‰
        with torch.no_grad():
            output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)
            pred_scaled = output[0, -1, 0].item()
        
        # ì—­ë³€í™˜
        close_idx = features.index('close')
        return inverse_transform_price(scaler, pred_scaled, close_idx)
        
    except Exception as e:
        if ticker in ['005930', '000660', '035720']:
            print(f"Error in run_inference for {ticker}: {str(e)}")
        return None
        
'''
def run_inference(ticker: str, model_info: Dict, df_recent: pd.DataFrame, next_trading_day: datetime, cfg: Config) -> Optional[float]:
    try:
        config_name = model_info['config_name']
        model_path = model_info['model_path']
        if not model_path.exists(): 
            return None
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint = torch.load(model_path, map_location=cfg.device, weights_only=False)
        
        # í•™ìŠµì‹œ ì‚¬ìš©ëœ feature ê°œìˆ˜ í™•ì¸
        saved_weight_shape = checkpoint['model_state_dict']['enc_embedding.value_embedding.tokenConv.weight'].shape
        trained_features_count = saved_weight_shape[1]  # 18
        
        with suppress_stdout():
            model_config = copy.deepcopy(MODEL_CONFIGS[config_name])
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì „ì²´ features ê°€ì ¸ì˜¤ê¸°
            with open(cfg.METADATA_DIR / f"{ticker}.json", 'r') as f: 
                metadata = json.load(f)
                all_features = metadata['features']
            
            # í•™ìŠµì‹œì™€ ë™ì¼í•œ ê°œìˆ˜ì˜ featuresë§Œ ì‚¬ìš© (ì²˜ìŒ 18ê°œ)
            features = all_features[:trained_features_count]
            
            # ëª¨ë¸ ì„¤ì •
            model_config.enc_in = model_config.dec_in = trained_features_count
            model_config.c_out = 1
            
            # FEDformer_base ëª¨ë¸ ìƒì„± (FEDformerWithEmbedding ì•„ë‹˜!)
            model = FEDformer_base(model_config).to(cfg.device)
            model.load_state_dict(checkpoint['model_state_dict'])
            model.eval()

        # Scaler ë¡œë“œ ë° í™•ì¸
        scaler = joblib.load(cfg.SCALER_DIR / f"{ticker}.pkl")
        
        # Scalerê°€ ê¸°ëŒ€í•˜ëŠ” feature ê°œìˆ˜ í™•ì¸
        if hasattr(scaler, 'n_features_in_'):
            if scaler.n_features_in_ != trained_features_count:
                # Scalerì˜ feature ê°œìˆ˜ê°€ ë‹¤ë¥¸ ê²½ìš°
                features = all_features[:scaler.n_features_in_]
        
        # ë°ì´í„° ì „ì²˜ë¦¬ - í•™ìŠµì‹œ ì‚¬ìš©ëœ featuresë§Œ ì„ íƒ
        data_to_scale = df_recent[features].astype(np.float32)
        scaled_data = scaler.transform(data_to_scale)
        
        # ì‹œê°„ íŠ¹ì§• ìƒì„±
        time_marks = time_features(pd.DatetimeIndex(df_recent['date']), freq=model_config.freq).transpose()
        
        # í…ì„œ ë³€í™˜
        x_enc = torch.tensor(scaled_data, dtype=torch.float32).unsqueeze(0).to(cfg.device)
        x_mark_enc = torch.tensor(time_marks, dtype=torch.float32).unsqueeze(0).to(cfg.device)
        
        # ë””ì½”ë” ì…ë ¥ ìƒì„±
        dec_inp_context = x_enc[:, -model_config.label_len:, :]
        dec_inp_zeros = torch.zeros(1, model_config.pred_len, model_config.dec_in, device=cfg.device)
        x_dec = torch.cat([dec_inp_context, dec_inp_zeros], dim=1)
        
        # ë””ì½”ë” ì‹œê°„ íŠ¹ì§•
        decoder_dates = pd.DatetimeIndex(df_recent['date'].iloc[-model_config.label_len:].tolist() + [next_trading_day])
        x_mark_dec = torch.tensor(time_features(decoder_dates, freq=model_config.freq).T, dtype=torch.float32).unsqueeze(0).to(cfg.device)
        
        # ì¶”ë¡  ì‹¤í–‰
        with torch.no_grad():
            # FEDformer_baseëŠ” 4ê°œ ì¸ìë§Œ ë°›ìŒ
            output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)
            pred_scaled = output[0, -1, 0].item()
        
        # ì—­ë³€í™˜
        close_idx = features.index('close')
        return inverse_transform_price(scaler, pred_scaled, close_idx)
        
    except Exception as e:
        # ë””ë²„ê¹…ìš© - ì²˜ìŒ ëª‡ ê°œ ì¢…ëª©ë§Œ ì—ëŸ¬ ì¶œë ¥
        if ticker in ['005930', '000660', '035720']:
            print(f"Error in run_inference for {ticker}: {str(e)}")
        return None       
'''


def test_inference():
    """run_inference í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*50)
    print("run_inference í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*50)
    
    cfg = Config()
    
    # í…ŒìŠ¤íŠ¸í•  ì¢…ëª© ì„ íƒ
    test_ticker = '005930'  # ì‚¼ì„±ì „ì
    
    # ëª¨ë¸ ì •ë³´ ì„¤ì •
    model_info = {
        'config_name': 'base',
        'model_path': cfg.V13_RESULT_DIR / "models" / f"model_{test_ticker}_base.pth"
    }
    
    print(f"í…ŒìŠ¤íŠ¸ ì¢…ëª©: {test_ticker}")
    print(f"ëª¨ë¸ ê²½ë¡œ: {model_info['model_path']}")
    print(f"ëª¨ë¸ ì¡´ì¬: {model_info['model_path'].exists()}")
    
    if not model_info['model_path'].exists():
        print("âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    data_handler = DataHandler(cfg.PROCESSED_DB_PATH, cfg.SOURCE_DB_PATH, cfg.DB_TIMEOUT)
    all_trading_dates = data_handler.get_all_trading_dates()
    
    # ìµœê·¼ 60ì¼ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    df_recent = data_handler.get_price_data_until(
        test_ticker, 
        all_trading_dates.iloc[-1].strftime('%Y-%m-%d'), 
        60
    )
    
    if df_recent is None:
        print("âŒ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
        
    print(f"ë°ì´í„° shape: {df_recent.shape}")
    
    # ë‹¤ìŒ ê±°ë˜ì¼ ê³„ì‚°
    next_trading_day = get_next_trading_day(df_recent['date'].iloc[-1], all_trading_dates)
    print(f"ë‹¤ìŒ ê±°ë˜ì¼: {next_trading_day}")
    
    # ì˜ˆì¸¡ ì‹¤í–‰ - run_inference í•¨ìˆ˜ë¥¼ ì§ì ‘ í˜¸ì¶œ
    print("\nì˜ˆì¸¡ ì‹¤í–‰ ì¤‘...")
    predicted_price = run_inference(test_ticker, model_info, df_recent, next_trading_day, cfg)
    
    if predicted_price is not None:
        print(f"âœ“ ìµœì¢… ì˜ˆì¸¡ê°€ê²©: {predicted_price:.2f}")
    else:
        print("âŒ ì˜ˆì¸¡ ì‹¤íŒ¨")
        

# ì´ í•¨ìˆ˜ ì „ì²´ë¥¼ ì•„ë˜ ë‚´ìš©ìœ¼ë¡œ êµì²´í•´ì£¼ì„¸ìš”.
def run_backtest_and_visualize(
    ticker: str, 
    ticker_name: str, 
    model_info: Dict, 
    all_trading_dates: pd.Series, 
    cfg: Config, 
    next_day_prediction: float, # <--- ì´ ì¸ìê°€ ëˆ„ë½ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
    backtest_days: int = 20
) -> Dict:
    """ì¶”ì²œëœ ì¢…ëª©ì˜ ê³¼ê±° Nì¼ê°„ ì˜ˆì¸¡ + ë¯¸ë˜ 1ì¼ ì˜ˆì¸¡ì„ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    
    data_handler = DataHandler(cfg.PROCESSED_DB_PATH, cfg.SOURCE_DB_PATH, cfg.DB_TIMEOUT)
    model_config = MODEL_CONFIGS[model_info['config_name']]
    
    end_date = all_trading_dates.iloc[-1]
    backtest_dates = all_trading_dates[all_trading_dates <= end_date].iloc[-backtest_days:]
    
    predictions, actuals, last_closes = [], [], []
    
    for date in backtest_dates:
        end_of_window = date - pd.Timedelta(days=1)
        df_recent = data_handler.get_price_data_until(ticker, end_of_window.strftime('%Y-%m-%d'), model_config.seq_len)
        
        if df_recent is None or len(df_recent) < model_config.seq_len:
            continue
            
        df_actual = data_handler.get_price_data_until(ticker, date.strftime('%Y-%m-%d'), 1)
        if df_actual is None or df_actual.empty:
            continue
        
        with open(cfg.METADATA_DIR / f"{ticker}.json") as f: features = json.load(f)['features']
        close_idx = features.index('close')
        
        predicted_price = run_inference(ticker, model_info, df_recent, date, cfg)
        if predicted_price is None:
            continue
        
        actual_price_log = df_actual.iloc[0][features].iloc[close_idx]
        last_close_log = df_recent.iloc[-1][features].iloc[close_idx]
        
        predictions.append(predicted_price)
        actuals.append(np.expm1(actual_price_log))
        last_closes.append(np.expm1(last_close_log))

    if not predictions:
        return {'backtest_f1': 0, 'backtest_nrmse': 1.0, 'plot_path': 'N/A'}
    
    y_true, y_pred, y_last = np.array(actuals), np.array(predictions), np.array(last_closes)
    true_dir, pred_dir = (y_true > y_last).astype(int), (y_pred > y_last).astype(int)
    f1 = f1_score(true_dir, pred_dir, zero_division=0)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    nrmse = rmse / (y_true.max() - y_true.min()) if (y_true.max() - y_true.min()) > 1e-5 else 0

    try:
        last_backtest_date = backtest_dates.iloc[-1]
        next_pred_date = get_next_trading_day(last_backtest_date, all_trading_dates)
        plot_dates = backtest_dates[-len(y_pred):].tolist() + [next_pred_date]
        plot_predictions = y_pred.tolist() + [next_day_prediction]

        plt.style.use('seaborn-v0_8-darkgrid')
        plt.rc('font', family='Malgun Gothic' if sys.platform.startswith('win') else 'AppleGothic')
        plt.rcParams['axes.unicode_minus'] = False

        fig, ax = plt.subplots(figsize=(12, 6))
        ax.plot(backtest_dates[-len(y_true):], y_true, label='ì‹¤ì œ ì£¼ê°€ (Actual Price)', marker='o', markersize=4, linestyle='-')
        ax.plot(plot_dates, plot_predictions, label='ì˜ˆì¸¡ ì£¼ê°€ (Predicted Price)', marker='x', markersize=4, linestyle='--')
        ax.set_title(f'ìµœê·¼ {backtest_days}ì¼ ë°±í…ŒìŠ¤íŠ¸: {ticker_name} ({ticker})', fontsize=16)
        ax.set_ylabel('ì¢…ê°€ (KRW)', fontsize=12)
        ax.legend(fontsize=12)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plot_path = cfg.BACKTEST_PLOTS_DIR / f"{ticker}_backtest.png"
        plt.savefig(plot_path)
        plt.close(fig)

    except Exception as e:
        print(f"Plotting Error for {ticker}: {e}")
        plot_path = "N/A"

    return {'backtest_f1': round(f1, 4), 'backtest_nrmse': round(nrmse, 4), 'plot_path': str(plot_path)}  
    

def predict_worker(args: Tuple) -> List[Dict]:
    ticker_info, dates_to_predict, all_trading_dates, cfg = args
    ticker = ticker_info['ticker']
    
    # ë””ë²„ê¹…: ì²˜ìŒ ëª‡ ê°œ ì¢…ëª©ë§Œ ì¶œë ¥
    if ticker in ['005930', '000660', '035720']:  # ì‚¼ì„±ì „ì, SKí•˜ì´ë‹‰ìŠ¤, ì¹´ì¹´ì˜¤ ë“±
        print(f"\n[DEBUG predict_worker] Processing {ticker}")
        print(f"  - dates_to_predict ìˆ˜: {len(dates_to_predict)}")
        if len(dates_to_predict) > 0:
            print(f"  - ì²« ë‚ ì§œ: {dates_to_predict.iloc[0]}")
            print(f"  - ë§ˆì§€ë§‰ ë‚ ì§œ: {dates_to_predict.iloc[-1]}")
    
    results = []
    
    try:
        data_handler = DataHandler(cfg.PROCESSED_DB_PATH, cfg.SOURCE_DB_PATH, cfg.DB_TIMEOUT)
        model_seq_len = MODEL_CONFIGS[ticker_info['config_name']].seq_len
        
        for base_date in dates_to_predict:
            base_date_str = base_date.strftime('%Y-%m-%d')
            
            df_recent = data_handler.get_price_data_until(ticker, base_date_str, model_seq_len)
            
            if df_recent is None or len(df_recent) < model_seq_len:
                results.append({'ticker': ticker, 'base_date': base_date_str, 'status': 'Failure'})
                continue

            last_known_date = df_recent['date'].iloc[-1]
            next_trading_day = get_next_trading_day(last_known_date, all_trading_dates)
            
            predicted_price = run_inference(ticker, ticker_info, df_recent, next_trading_day, cfg)
            
            if predicted_price is None:
                results.append({'ticker': ticker, 'base_date': base_date_str, 'status': 'Failure'})
                continue
                
            with open(cfg.METADATA_DIR / f"{ticker}.json") as f: 
                features = json.load(f)['features']
            close_idx = features.index('close')
            last_close_log = df_recent.iloc[-1][features].iloc[close_idx]
            last_close_price = np.expm1(last_close_log)
            
            results.append({
                'ticker': ticker,
                'base_date': base_date_str,
                'prediction_date': next_trading_day.strftime('%Y-%m-%d'),
                'last_close': last_close_price,
                'predicted_close': predicted_price,
                'expected_return': ((predicted_price / last_close_price) - 1) * 100 if last_close_price > 0 else 0,
                'status': 'Success',
                # ì—¬ê¸°ì— f1ê³¼ nrmse ì¶”ê°€!
                'f1': ticker_info.get('f1', 0),
                'nrmse': ticker_info.get('nrmse', 1.0),
                'config_name': ticker_info['config_name'],
                'model_path': str(ticker_info['model_path']),
                'name': ticker_info.get('name', 'N/A'),
                'market': ticker_info.get('market', 'N/A'),
                'sector': ticker_info.get('sector', 'Unknown')
            })
            
        return results
    except Exception as e:
        return [{'ticker': ticker, 'status': 'Failure'}]
    finally:
        if 'cuda' in str(cfg.device): 
            gc.collect()
            torch.cuda.empty_cache()

# ==================================================
# 6. ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ==================================================
def find_best_models_across_versions(cfg: Config, stock_meta: Dict) -> List[Dict]:
    print("\n" + "="*20 + " STEP 1: v13 & v14 ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í†µí•© ì„ ì • " + "="*20)
    
    log_files = {'v13': cfg.V13_RESULT_DIR / "performance_log.csv", 'v14': cfg.V14_RESULT_DIR / "performance_log.csv"}
    all_logs = []
    for version, path in log_files.items():
        if path.exists():
            df = pd.read_csv(path)
            df['ticker'] = df['ticker'].astype(str) # <--- ì´ ì¤„ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.
            df['version'] = version
            all_logs.append(df)
            print(f"  âœ“ {version} ë¡œê·¸ ë¡œë“œ ì™„ë£Œ ({len(df)}ê°œ ê¸°ë¡)")
        else:
            print(f"  - {version} ë¡œê·¸ íŒŒì¼ ì—†ìŒ. ê±´ë„ˆëœë‹ˆë‹¤.")

    if not all_logs: sys.exit("âŒ ë¶„ì„í•  ì„±ëŠ¥ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
    combined_df = pd.concat(all_logs, ignore_index=True)
    stock_df = combined_df[combined_df['model_type'] == 'stock'].copy()
    
    stock_df_sorted = stock_df.sort_values(by=['f1', 'nrmse'], ascending=[False, True])
    best_models_df = stock_df_sorted.drop_duplicates('ticker', keep='first')
    
    targets = []
    for _, row in best_models_df.iterrows():
        version = row['version']
        models_dir = cfg.V13_RESULT_DIR / "models" if version == 'v13' else cfg.V14_RESULT_DIR / "models"
        ticker_meta = stock_meta.get(row['ticker'], {})
        
        targets.append({
            'ticker': row['ticker'],
            'name': ticker_meta.get('name', 'N/A'),
            'market': ticker_meta.get('market', 'N/A'),
            'sector': ticker_meta.get('sector', 'Unknown'),
            'config_name': row['config_name'],
            'f1': row['f1'],
            'nrmse': row['nrmse'],
            'version': version,
            'model_path': models_dir / f"model_{row['ticker']}_{row['config_name']}.pth"
        })
        
    print(f"âœ… ì´ {len(targets)}ê°œ ì¢…ëª©ì— ëŒ€í•œ ìµœì¢… ë² ìŠ¤íŠ¸ ëª¨ë¸ ì„ ì •ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
    return targets

def predict_market_indices(cfg: Config, all_trading_dates: pd.Series) -> Tuple[str, pd.DataFrame, float]:
    print("\n" + "="*20 + " STEP 2: ì‹œì¥ ì§€ìˆ˜ ì˜ˆì¸¡ ë° ë°©í–¥ì„± íŒë‹¨ " + "="*20)
    index_predictions = []
    
    for ticker in ['KOSPI', 'KOSDAQ']:
        model_info = {
            'config_name': 'base', 
            'model_path': cfg.V13_RESULT_DIR / "models" / f"model_{ticker}_base.pth"
        }
        
        # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not model_info['model_path'].exists():
            print(f"  - {ticker} ëª¨ë¸ íŒŒì¼ ì—†ìŒ. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
            
        data_handler = DataHandler(cfg.PROCESSED_DB_PATH, cfg.SOURCE_DB_PATH, cfg.DB_TIMEOUT)
        df_recent = data_handler.get_price_data_until(
            ticker, 
            all_trading_dates.iloc[-1].strftime('%Y-%m-%d'), 
            MODEL_CONFIGS['base'].seq_len
        )
        
        if df_recent is None:
            print(f"  - {ticker} ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ì˜ˆì¸¡ ì‹¤íŒ¨.")
            continue
            
        last_known_date = df_recent['date'].iloc[-1]
        next_trading_day = get_next_trading_day(last_known_date, all_trading_dates)
        
        # run_inference ì‚¬ìš©
        predicted_price = run_inference(ticker, model_info, df_recent, next_trading_day, cfg)
        
        if predicted_price:
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œí•´ì„œ í•™ìŠµì‹œ feature ê°œìˆ˜ í™•ì¸
            checkpoint = torch.load(model_info['model_path'], map_location=cfg.device, weights_only=False)
            trained_features_count = checkpoint['model_state_dict']['enc_embedding.value_embedding.tokenConv.weight'].shape[1]
            
            with open(cfg.METADATA_DIR / f"{ticker}.json") as f: 
                all_features = json.load(f)['features']
            
            features = all_features[:trained_features_count]
            close_idx = features.index('close')
            last_close_log = df_recent.iloc[-1][features].iloc[close_idx]
            last_close_price = np.expm1(last_close_log)
            expected_return = ((predicted_price / last_close_price) - 1) * 100
            index_predictions.append({'ticker': ticker, 'predicted_return': expected_return})
    
    if not index_predictions:
        print("  - ì‹œì¥ ì§€ìˆ˜ ì˜ˆì¸¡ ì‹¤íŒ¨. 'ë³´í•©(Neutral)'ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.")
        return 'Neutral', pd.DataFrame(), 0.0
        
    df_index = pd.DataFrame(index_predictions)
    avg_return = df_index['predicted_return'].mean()
    
    market_sentiment = 'Neutral'
    if avg_return >= cfg.MARKET_UP_THRESHOLD: 
        market_sentiment = 'Bullish'
    elif avg_return <= cfg.MARKET_DOWN_THRESHOLD: 
        market_sentiment = 'Bearish'
        
    print(f"  - í‰ê·  ì˜ˆìƒ ìˆ˜ìµë¥ : {avg_return:.2f}%")
    print(f"  - ì‹œì¥ ë°©í–¥ì„± íŒë‹¨: {market_sentiment}")
    
    return market_sentiment, df_index, avg_return

def diversify_by_sector(df: pd.DataFrame, max_per_sector: int) -> pd.DataFrame:
    if 'sector' not in df.columns or df['sector'].nunique() <= 1:
        return df
    
    diversified_list = []
    sector_count = {}
    df_copy = df.copy()
    for index, row in df_copy.iterrows():
        sector = row.get('sector', 'Unknown')
        if sector_count.get(sector, 0) < max_per_sector:
            diversified_list.append(row)
            sector_count[sector] = sector_count.get(sector, 0) + 1
    return pd.DataFrame(diversified_list)
    
def get_prediction_for_date(ticker: str, model_info: Dict, target_date: datetime, all_trading_dates: pd.Series, cfg: Config) -> Optional[float]:
    """
    íŠ¹ì • 'target_date'ì˜ ì¢…ê°€ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    (ë‚´ë¶€ì ìœ¼ë¡œ target_dateì˜ 'ì´ì „ ê±°ë˜ì¼'ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤)
    """
    data_handler = DataHandler(cfg.PROCESSED_DB_PATH, cfg.SOURCE_DB_PATH, cfg.DB_TIMEOUT)
    model_seq_len = MODEL_CONFIGS[model_info['config_name']].seq_len
    
    # target_dateì˜ ì´ì „ ê±°ë˜ì¼ ì°¾ê¸°
    # all_trading_datesëŠ” ì •ë ¬ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
    try:
        # np.whereë¥¼ ì‚¬ìš©í•˜ì—¬ target_dateì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ê³  1ì„ ë¹¼ì„œ ì´ì „ ì¸ë±ìŠ¤ë¥¼ êµ¬í•©ë‹ˆë‹¤.
        prev_day_index = np.where(all_trading_dates.to_numpy() == np.datetime64(target_date))[0][0] - 1
        if prev_day_index < 0:
            return None
        prev_trading_day = all_trading_dates.iloc[prev_day_index]
    except IndexError:
        # target_dateê°€ ë¦¬ìŠ¤íŠ¸ì— ì—†ëŠ” ê²½ìš° ë“± ì˜ˆì™¸ ì²˜ë¦¬
        return None
    
    # ì´ì „ ê±°ë˜ì¼ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
    df_recent = data_handler.get_price_data_until(ticker, prev_trading_day.strftime('%Y-%m-%d'), model_seq_len)
    if df_recent is None or len(df_recent) < model_seq_len:
        return None
        
    # ì˜ˆì¸¡ ì‹¤í–‰ (ì˜ˆì¸¡ ëª©í‘œì¼ì€ target_dateê°€ ë©ë‹ˆë‹¤)
    predicted_price = run_inference(ticker, model_info, df_recent, target_date, cfg)
    return predicted_price

def main():
    start_time = datetime.now()
    print(f"ğŸš€ FEDformer PPO ë°ì´í„° ìƒì„± ë° ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    cfg = Config()
    print(f"â–¶ï¸ Using device: {cfg.device}")
    
    data_handler = DataHandler(cfg.PROCESSED_DB_PATH, cfg.SOURCE_DB_PATH, cfg.DB_TIMEOUT)
    stock_metadata = data_handler.get_stock_metadata()
    all_trading_dates = data_handler.get_all_trading_dates()

    if all_trading_dates.empty:
        sys.exit("âŒ 'processed_daily_prices' í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

    # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ì´ ë¶€ë¶„ ì „ì²´ ìˆ˜ì • â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
    print("\n" + "="*20 + " ì˜ˆì¸¡ ê¸°ê°„ ì„¤ì • " + "="*20)
    
    # DBì˜ ì‹¤ì œ ìµœì‹  ë‚ ì§œ í™•ì¸
    latest_date = all_trading_dates.iloc[-1]
    earliest_date = all_trading_dates.iloc[0]
    
    print(f"DB ë°ì´í„° ë²”ìœ„: {earliest_date.date()} ~ {latest_date.date()}")
    latest_date_str = latest_date.strftime('%Y-%m-%d')
    
    # ì‚¬ìš©ì ì…ë ¥
    start_date_input = input(f"â–¶ï¸ ì˜ˆì¸¡ ì‹œì‘ì¼(YYYY-MM-DD) ì…ë ¥ (ë¯¸ì…ë ¥ ì‹œ ìµœì‹ ì¼ {latest_date_str}): ").strip()
    end_date_input = input(f"â–¶ï¸ ì˜ˆì¸¡ ì¢…ë£Œì¼(YYYY-MM-DD) ì…ë ¥ (ë¯¸ì…ë ¥ ì‹œ ìµœì‹ ì¼ {latest_date_str}): ").strip()
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    start_date_str = start_date_input if start_date_input else latest_date_str
    end_date_str = end_date_input if end_date_input else latest_date_str
    
    # ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
    try:
        start_date_dt = pd.to_datetime(start_date_str)
        end_date_dt = pd.to_datetime(end_date_str)
    except ValueError as e:
        sys.exit(f"âŒ ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜: {e}")
    
    # ========== í•µì‹¬ ìˆ˜ì •: ê±°ë˜ì¼ì´ ì•„ë‹Œ ë‚ ì§œ ì²˜ë¦¬ ==========
    # ì‹œì‘ì¼ì´ ê±°ë˜ì¼ì´ ì•„ë‹Œ ê²½ìš°, ê·¸ ì´ì „ì˜ ê°€ì¥ ê°€ê¹Œìš´ ê±°ë˜ì¼ë¡œ ì¡°ì •
    if start_date_dt not in all_trading_dates.values:
        prev_trading_dates = all_trading_dates[all_trading_dates <= start_date_dt]
        if prev_trading_dates.empty:
            print(f"âŒ {start_date_str} ì´ì „ì— ê±°ë˜ì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        adjusted_start = prev_trading_dates.iloc[-1]
        print(f"âš ï¸  {start_date_dt.date()}ëŠ” ê±°ë˜ì¼ì´ ì•„ë‹™ë‹ˆë‹¤.")
        print(f"   â†’ ì´ì „ ê±°ë˜ì¼ì¸ {adjusted_start.date()}ë¡œ ìë™ ì¡°ì •")
        start_date_dt = adjusted_start
        start_date_str = adjusted_start.strftime('%Y-%m-%d')

    # ì¢…ë£Œì¼ì´ ê±°ë˜ì¼ì´ ì•„ë‹Œ ê²½ìš°, ê·¸ ì´ì „ì˜ ê°€ì¥ ê°€ê¹Œìš´ ê±°ë˜ì¼ë¡œ ì¡°ì •
    if end_date_dt not in all_trading_dates.values:
        prev_trading_dates = all_trading_dates[all_trading_dates <= end_date_dt]
        if prev_trading_dates.empty:
            print(f"âŒ {end_date_str} ì´ì „ì— ê±°ë˜ì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        adjusted_end = prev_trading_dates.iloc[-1]
        print(f"âš ï¸  {end_date_dt.date()}ëŠ” ê±°ë˜ì¼ì´ ì•„ë‹™ë‹ˆë‹¤.")
        print(f"   â†’ ì´ì „ ê±°ë˜ì¼ì¸ {adjusted_end.date()}ë¡œ ìë™ ì¡°ì •")
        end_date_dt = adjusted_end
        end_date_str = adjusted_end.strftime('%Y-%m-%d')
        
        
    # ì…ë ¥ëœ ë‚ ì§œê°€ DB ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ”ì§€ í™•ì¸
    if start_date_dt > latest_date:
        print(f"âš ï¸  ê²½ê³ : ì‹œì‘ì¼({start_date_dt.date()})ì´ DB ìµœì‹ ì¼({latest_date.date()})ë³´ë‹¤ ë¯¸ë˜ì…ë‹ˆë‹¤.")
        print(f"   -> ìµœì‹ ì¼ë¡œ ìë™ ì¡°ì •í•©ë‹ˆë‹¤.")
        start_date_dt = latest_date
        start_date_str = latest_date_str
    
    if end_date_dt > latest_date:
        print(f"âš ï¸  ê²½ê³ : ì¢…ë£Œì¼({end_date_dt.date()})ì´ DB ìµœì‹ ì¼({latest_date.date()})ë³´ë‹¤ ë¯¸ë˜ì…ë‹ˆë‹¤.")
        print(f"   -> ìµœì‹ ì¼ë¡œ ìë™ ì¡°ì •í•©ë‹ˆë‹¤.")
        end_date_dt = latest_date
        end_date_str = latest_date_str
    
    # datetime íƒ€ì…ìœ¼ë¡œ í•„í„°ë§ (ë¬¸ìì—´ ë¹„êµ X)
    dates_to_predict = all_trading_dates[
        (all_trading_dates >= start_date_dt) & 
        (all_trading_dates <= end_date_dt)
    ]
    
    if dates_to_predict.empty:
        print(f"âŒ {start_date_str} ~ {end_date_str} ê¸°ê°„ì— ê±°ë˜ì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   DB ë°ì´í„° ë²”ìœ„ë¥¼ í™•ì¸í•˜ì„¸ìš”: {earliest_date.date()} ~ {latest_date.date()}")
        sys.exit(1)
    
    print(f"âœ… ì˜ˆì¸¡ ëŒ€ìƒ ê¸°ê°„: {dates_to_predict.iloc[0].date()} ~ {dates_to_predict.iloc[-1].date()} ({len(dates_to_predict)} ê±°ë˜ì¼)")
    # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² ì—¬ê¸°ê¹Œì§€ ìˆ˜ì • â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²

    target_models = find_best_models_across_versions(cfg, stock_metadata)

    target_models = find_best_models_across_versions(cfg, stock_metadata)
    
    print("\n" + "="*20 + " STEP 2: ê¸°ê°„ ì˜ˆì¸¡ ë³‘ë ¬ ì‹¤í–‰ " + "="*20)
    
    # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ì´ ë¶€ë¶„ ìˆ˜ì • â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
    # ì¸ìì— cfgë¥¼ ì¶”ê°€
    worker_args = [(info, dates_to_predict, all_trading_dates, cfg) for info in target_models]
    # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² ì—¬ê¸°ê¹Œì§€ ìˆ˜ì • â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
    
    #worker_args = [(info, dates_to_predict, all_trading_dates) for info in target_models]

    all_results_list = []
    with mp.Pool(processes=cfg.MAX_CONCURRENT_PROCESSES) as pool:
        for res_list in tqdm(pool.imap_unordered(predict_worker, worker_args), total=len(worker_args), desc="   ì¢…ëª©ë³„ ê¸°ê°„ ì˜ˆì¸¡"):
            if res_list: all_results_list.extend(res_list)
            
    print("\nâœ… ëª¨ë“  ê¸°ê°„ ì˜ˆì¸¡ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    if not all_results_list:
        print("â„¹ï¸ ìœ íš¨í•œ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."); return
        
    print("\n" + "="*20 + " STEP 3: PPOìš© DB ì €ì¥ ë° ì¶”ì²œì£¼ ì„ ì • " + "="*20)
    df_results = pd.DataFrame(all_results_list)
    
    try:
            # reason ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì œê±°
        if 'reason' in df_results.columns:
            df_results = df_results.drop(columns=['reason'])
        with sqlite3.connect(cfg.PPO_DB_PATH) as conn:
            df_results.to_sql('predictions', conn, if_exists='append', index=False)
            conn.execute("CREATE INDEX IF NOT EXISTS idx_ticker_date ON predictions (ticker, base_date)")
            conn.execute("""
                DELETE FROM predictions
                WHERE rowid NOT IN (
                    SELECT MIN(rowid)
                    FROM predictions
                    GROUP BY ticker, base_date
                )
            """)
        print(f"  âœ“ ì´ {len(df_results)}ê±´ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ PPOìš© DB '{cfg.PPO_DB_PATH.name}'ì— ëˆ„ì  ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"  âœ— PPOìš© DB ì €ì¥ ì‹¤íŒ¨: {e}")

    # --- ìµœì¢…ì¼ ê¸°ì¤€ ì¶”ì²œ ë¡œì§ ---
    df_last_day_preds = df_results[df_results['base_date'] == end_date_str].copy()

    # ë””ë²„ê¹… ì¶œë ¥ ì¶”ê°€
    print(f"\n[DEBUG] ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ ìˆ˜: {len(df_results)}")
    print(f"[DEBUG] end_date_str: {end_date_str}")
    print(f"[DEBUG] df_resultsì˜ ê³ ìœ  base_date: {df_results['base_date'].unique()[:5]}...")  # ì²˜ìŒ 5ê°œë§Œ
    print(f"[DEBUG] ìµœì¢…ì¼({end_date_str}) ì˜ˆì¸¡ ê²°ê³¼ ìˆ˜: {len(df_last_day_preds)}")

    if not df_last_day_preds.empty:
        print(f"[DEBUG] df_last_day_preds columns: {df_last_day_preds.columns.tolist()}")
        print(f"[DEBUG] status ë¶„í¬: {df_last_day_preds['status'].value_counts().to_dict()}")
        
        # ì‹¤íŒ¨í•œ ê²½ìš° ì´ìœ  í™•ì¸
        if 'reason' in df_last_day_preds.columns:
            print(f"[DEBUG] ì‹¤íŒ¨ ì´ìœ  ë¶„í¬: {df_last_day_preds[df_last_day_preds['status']=='Failure']['reason'].value_counts().head()}")

    if df_last_day_preds.empty:
        print("  - ìµœì¢…ì¼ì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ì–´ ì¶”ì²œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
        
    market_sentiment, df_index_pred, avg_market_return = predict_market_indices(cfg, all_trading_dates)
    if market_sentiment == 'Bearish':
        print("\n" + "!"*50); print("âš ï¸  ê²½ê³ : ì‹œì¥ í•˜ë½ì´ ì˜ˆìƒë©ë‹ˆë‹¤. ë³´ìˆ˜ì ì¸ íˆ¬ìë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."); print("!"*50)
    
    df_success = df_last_day_preds[df_last_day_preds['status'] == 'Success'].copy()

    # df_successì— ì´ë¯¸ í•„ìš”í•œ ì»¬ëŸ¼ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
    print(f"\n[DEBUG] df_success columns: {df_success.columns.tolist()}")
    print(f"[DEBUG] df_success shape: {df_success.shape}")
    if not df_success.empty:
        print(f"[DEBUG] Sample f1 values: {df_success['f1'].head() if 'f1' in df_success.columns else 'f1 column missing'}")
        print(f"[DEBUG] Sample nrmse values: {df_success['nrmse'].head() if 'nrmse' in df_success.columns else 'nrmse column missing'}")
    

    df_success['avg_volume_20d'] = df_success['ticker'].apply(lambda t: DataHandler(cfg.PROCESSED_DB_PATH, cfg.SOURCE_DB_PATH, cfg.DB_TIMEOUT).get_avg_volume(t))
    df_filtered = df_success[df_success['avg_volume_20d'] > cfg.MIN_AVG_VOLUME].copy()
    
    # NRMSEì™€ F1 Score ê¸°ì¤€ìœ¼ë¡œ ì¶”ê°€ í•„í„°ë§
    nrmse_threshold = 0.25
    f1_threshold = 0.52

    print(f"\n-> [1ì°¨ í•„í„°ë§] NRMSE < {nrmse_threshold}, F1 Score >= {f1_threshold}")
    df_filtered = df_success[(df_success['nrmse'] < nrmse_threshold) & (df_success['f1'] >= f1_threshold)]
    print(f"-> 1ì°¨ í•„í„°ë§ í›„: {len(df_filtered)}ê°œ ì¢…ëª©")

    # ìµœì†Œ ì¶”ì²œ ê°œìˆ˜ ì„¤ì •
    MIN_RECOMMENDATIONS = 3

    # í•„í„°ë§ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì¡°ê±´ ì™„í™”
    if len(df_filtered) < MIN_RECOMMENDATIONS:
        print(f"\nâš ï¸  1ì°¨ í•„í„°ë§ í†µê³¼ ì¢…ëª©ì´ {len(df_filtered)}ê°œë¡œ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        print(f"   -> ì¡°ê±´ì„ ì™„í™”í•˜ì—¬ ìµœì†Œ {MIN_RECOMMENDATIONS}ê°œ ì¶”ì²œ")
        
        # 2ì°¨ ì™„í™”: NRMSE < 0.30, F1 >= 0.50
        nrmse_threshold_relaxed = 0.30
        f1_threshold_relaxed = 0.50
        
        print(f"   [2ì°¨ í•„í„°ë§] NRMSE < {nrmse_threshold_relaxed}, F1 >= {f1_threshold_relaxed}")
        df_filtered = df_success[
            (df_success['nrmse'] < nrmse_threshold_relaxed) & 
            (df_success['f1'] >= f1_threshold_relaxed)
        ]
        print(f"   -> 2ì°¨ í•„í„°ë§ í›„: {len(df_filtered)}ê°œ ì¢…ëª©")
        
        # ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ F1 Score ìƒìœ„ Nê°œ ì„ íƒ
        if len(df_filtered) < MIN_RECOMMENDATIONS:
            print(f"\nâš ï¸  2ì°¨ í•„í„°ë§ë„ ë¶€ì¡± ({len(df_filtered)}ê°œ)")
            print(f"   -> F1 Score ìƒìœ„ {MIN_RECOMMENDATIONS}ê°œë¥¼ ê°•ì œ ì„ íƒ (âš ï¸ ì„±ëŠ¥ ì£¼ì˜)")
            
            df_filtered = df_success.sort_values('f1', ascending=False).head(MIN_RECOMMENDATIONS * 2)
            print(f"   -> ìµœì¢…: {len(df_filtered)}ê°œ ì¢…ëª© (ì„±ëŠ¥ ê²½ê³  í¬í•¨)")
    

    # --- ì˜ˆì¸¡ ì¶”ì„¸ ê¸°ë°˜ ìˆ˜ìµë¥  ê³„ì‚° ---
    print("\n-> ì˜ˆì¸¡ ì¶”ì„¸ ê¸°ë°˜ ìˆ˜ìµë¥  ê³„ì‚° ì¤‘...")
    pred_trend_returns = []
    end_date_dt = pd.to_datetime(end_date_str)

    for _, row in tqdm(df_filtered.iterrows(), total=len(df_filtered), desc="    ì˜ˆì¸¡ ì¶”ì„¸ ê³„ì‚°"):
        pred_t_plus_1 = row['predicted_close']
        
        # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ [ì´ ë¶€ë¶„ ìˆ˜ì •] â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
        model_info = {
            'config_name': row['config_name'], 
            'model_path': Path(row['model_path']) # ğŸ‘ˆ strì„ Path ê°ì²´ë¡œ ë‹¤ì‹œ ë³€í™˜
        }
        # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² [ì—¬ê¸°ê¹Œì§€ ìˆ˜ì •] â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
        
        pred_t = get_prediction_for_date(row['ticker'], model_info, end_date_dt, all_trading_dates, cfg)
        
        if pred_t is not None and pred_t > 0:
            trend_return = ((pred_t_plus_1 / pred_t) - 1) * 100
            pred_trend_returns.append(trend_return)
        else:
            pred_trend_returns.append(0)

    df_filtered = df_filtered.copy()  # ë³µì‚¬ë³¸ ìƒì„±
    df_filtered['pred_trend_return'] = pred_trend_returns
    

    # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ì´ ë¶€ë¶„ ì „ì²´ ìˆ˜ì • â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
    # --- 1ë‹¨ê³„: ìƒìŠ¹ ì¶”ì„¸ í•„í„°ë§ (ê¸°ë³¸ ì¡°ê±´) ---
    print("\n-> 1ë‹¨ê³„: ìƒìŠ¹ ì¶”ì„¸ ì˜ˆì¸¡ ì¢…ëª© í•„í„°ë§...")
    df_uptrend = df_filtered[df_filtered['pred_trend_return'] > 0].copy()
    print(f"   âœ“ ìƒìŠ¹ ì¶”ì„¸ í•„í„°ë§ í›„: {len(df_uptrend)}ê°œ ì¢…ëª©")
    
    if df_uptrend.empty:
        print("   âš ï¸  ìƒìŠ¹ì´ ì˜ˆìƒë˜ëŠ” ì¢…ëª©ì´ ì—†ì–´ ì¶”ì²œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # --- 2ë‹¨ê³„: Confidence(F1 Score) ê¸°ì¤€ ì •ë ¬ ---
    print("\n-> 2ë‹¨ê³„: ì‹ ë¢°ë„(F1 Score) ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬...")
    # F1 Scoreê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬ (ë™ì ì¼ ê²½ìš° pred_trend_returnìœ¼ë¡œ 2ì°¨ ì •ë ¬)
    df_sorted = df_uptrend.sort_values(
        by=['f1', 'pred_trend_return'], 
        ascending=[False, False]
    )
    
    # Confidence ë“±ê¸‰ ë¶€ì—¬ (ì •ë ¬ ì „ì— ë¯¸ë¦¬ ê³„ì‚°)
    df_sorted['confidence'] = df_sorted['f1'].apply(
        lambda x: 'â­â­â­' if x >= 0.60 else 'â­â­' if x >= 0.55 else 'â­'
    )
    
    # ìƒìœ„ confidence ì¢…ëª©ë“¤ë§Œ ë¨¼ì € ì„ ë³„
    print(f"   - â­â­â­(F1â‰¥0.60): {len(df_sorted[df_sorted['f1'] >= 0.60])}ê°œ")
    print(f"   - â­â­  (F1â‰¥0.55): {len(df_sorted[df_sorted['f1'] >= 0.55])}ê°œ")
    print(f"   - â­   (F1<0.55): {len(df_sorted[df_sorted['f1'] < 0.55])}ê°œ")
    
    # --- 3ë‹¨ê³„: ì‹œì¥ ìƒí™©ì— ë”°ë¼ ì¶”ì²œ ê°œìˆ˜ ê²°ì • ---
    if market_sentiment == 'Bullish': 
        num_recommend = cfg.RECOMMEND_COUNT_BULL
    elif market_sentiment == 'Bearish': 
        num_recommend = cfg.RECOMMEND_COUNT_BEAR
    else: 
        num_recommend = cfg.RECOMMEND_COUNT_NEUTRAL
    
    print(f"\n-> 3ë‹¨ê³„: ì‹œì¥ ìƒí™©({market_sentiment})ì— ë”°ë¥¸ ì¶”ì²œ ê°œìˆ˜: {num_recommend}ê°œ")
    
    # --- 4ë‹¨ê³„: ì„¹í„° ë¶„ì‚°ì„ ê³ ë ¤í•œ ìµœì¢… ì„ ì • ---
    # ì •ë ¬ëœ ìƒìœ„ ì¢…ëª© ì¤‘ì—ì„œ ì„¹í„° ë¶„ì‚° ì ìš© (ì—¬ìœ ìˆê²Œ 2ë°° ì„ íƒ)
    df_top_candidates = df_sorted.head(num_recommend * 3)  # 3ë°°ë¡œ ëŠ˜ë ¤ì„œ ì„ íƒ í­ í™•ëŒ€
    
    print(f"\n-> 4ë‹¨ê³„: ì„¹í„° ë¶„ì‚° ì ìš© (í›„ë³´: {len(df_top_candidates)}ê°œ)")
    df_recommended = diversify_by_sector(
        df_top_candidates, 
        max_per_sector=cfg.MAX_PER_SECTOR
    ).copy()
    
    # ìµœì¢… ê°œìˆ˜ë§Œí¼ë§Œ ì„ íƒ
    df_recommended = df_recommended.head(num_recommend)
    
    # ì„¹í„°ë³„ ë¶„í¬ ì¶œë ¥
    if not df_recommended.empty:
        sector_dist = df_recommended['sector'].value_counts()
        print(f"   âœ“ ìµœì¢… ì¶”ì²œ ì¢…ëª©ì˜ ì„¹í„° ë¶„í¬:")
        for sector, count in sector_dist.items():
            print(f"      - {sector}: {count}ê°œ")
    # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² ì—¬ê¸°ê¹Œì§€ ìˆ˜ì • â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
    
    print(f"\nâœ… ì‹œì¥ ìƒí™©({market_sentiment}) ë° ì‹ ë¢°ë„ ê¸°ì¤€ìœ¼ë¡œ ìµœì¢… {len(df_recommended)}ê°œ ì¢…ëª©ì„ ì¶”ì²œí•©ë‹ˆë‹¤.")
    
    if not df_recommended.empty:
        print("\n" + "="*20 + " STEP 4: ì¶”ì²œì£¼ ë°±í…ŒìŠ¤íŒ… ë° ì‹œê°í™” " + "="*20)
        
        # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ë°±í…ŒìŠ¤íŒ… ë¨¼ì € ì‹¤í–‰ â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
        backtest_results = []
        for _, row in tqdm(df_recommended.iterrows(), total=len(df_recommended), desc="    ì¶”ì²œì£¼ ë°±í…ŒìŠ¤íŒ…"):
            
            # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ [ì´ ë¶€ë¶„ ìˆ˜ì •] â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
            model_info = {
                'config_name': row['config_name'], 
                'model_path': Path(row['model_path']) # ğŸ‘ˆ strì„ Path ê°ì²´ë¡œ ë‹¤ì‹œ ë³€í™˜
            }
            # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² [ì—¬ê¸°ê¹Œì§€ ìˆ˜ì •] â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
            
            result = run_backtest_and_visualize(
                row['ticker'], 
                row['name'], 
                model_info, 
                all_trading_dates, 
                cfg, 
                row['predicted_close'],
                backtest_days=20
            )
            result['ticker'] = row['ticker']
            backtest_results.append(result)

        if backtest_results:
            df_backtest = pd.DataFrame(backtest_results)
            df_recommended = pd.merge(df_recommended, df_backtest, on='ticker', how='left')
        
        # Confidence ë“±ê¸‰ ë¶€ì—¬
        df_recommended['confidence'] = df_recommended['f1'].apply(
            lambda x: 'â­â­â­' if x >= 0.60 else 'â­â­' if x >= 0.55 else 'â­'
        )
        # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² ì—¬ê¸°ê¹Œì§€ ë°±í…ŒìŠ¤íŒ… â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
        
        # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ì´ ë¶€ë¶„ ì „ì²´ ì¶”ê°€ â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
        print("\n" + "="*20 + " STEP 5: ë°±í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ê¸°ë°˜ ìµœì¢… ì„ ì • " + "="*20)
        
        # --- 1ë‹¨ê³„: ë°±í…ŒìŠ¤íŠ¸ F1ê³¼ NRMSE ê¸°ì¤€ìœ¼ë¡œ ì¬í•„í„°ë§ ---
        print("\n-> 1ë‹¨ê³„: ë°±í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ í•„í„°ë§...")
        
        # STEP 5ì˜ í•„í„°ë§ ê¸°ì¤€ ë¶€ë¶„ ìˆ˜ì •

        print("\n" + "="*20 + " STEP 5: ë°±í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ê¸°ë°˜ ìµœì¢… ì„ ì • " + "="*20)
        
        # ë°±í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ê¸°ì¤€ ì„¤ì •
        backtest_f1_threshold = 0.55     
        backtest_nrmse_threshold = 0.15  
        relaxed_f1_threshold = 0.50      
        relaxed_nrmse_threshold = 0.20   
        
        print(f"\n-> 1ë‹¨ê³„: ë°±í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ í•„í„°ë§...")
        print(f"   [ì—„ê²© ê¸°ì¤€] backtest_f1 >= {backtest_f1_threshold}, backtest_nrmse <= {backtest_nrmse_threshold}")
        
        df_backtest_filtered = df_recommended[
            (df_recommended['backtest_f1'] >= backtest_f1_threshold) & 
            (df_recommended['backtest_nrmse'] <= backtest_nrmse_threshold)
        ].copy()
        
        print(f"   âœ“ ì—„ê²© ê¸°ì¤€ í†µê³¼: {len(df_backtest_filtered)}ê°œ ì¢…ëª©")
        
        # ëª©í‘œ ê°œìˆ˜
        if market_sentiment == 'Bullish': 
            num_final = cfg.RECOMMEND_COUNT_BULL
        elif market_sentiment == 'Bearish': 
            num_final = cfg.RECOMMEND_COUNT_BEAR
        else: 
            num_final = cfg.RECOMMEND_COUNT_NEUTRAL
        
        # 1ë‹¨ê³„: ì—„ê²© ê¸°ì¤€ìœ¼ë¡œ ë¶€ì¡±í•˜ë©´ ì¶”ê°€ ë°±í…ŒìŠ¤íŒ…
        if len(df_backtest_filtered) < num_final:
            print(f"\n   ğŸ“Š ì—„ê²© ê¸°ì¤€ í†µê³¼ ì¢…ëª©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ì¶”ê°€ ë°±í…ŒìŠ¤íŒ…ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
            
            already_tested = set(df_recommended['ticker'].tolist())
            remaining_candidates = df_sorted[~df_sorted['ticker'].isin(already_tested)].head(num_final * 3)
            
            if not remaining_candidates.empty:
                print(f"      -> ì¶”ê°€ {len(remaining_candidates)}ê°œ ì¢…ëª© ë°±í…ŒìŠ¤íŒ… ì¤‘...")
                
                additional_backtest_results = []
                for _, row in tqdm(remaining_candidates.iterrows(), 
                   total=len(remaining_candidates), 
                   desc="    ì¶”ê°€ ë°±í…ŒìŠ¤íŒ…"):
                    
                    # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ [ì´ ë¶€ë¶„ ìˆ˜ì •] â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
                    model_info = {
                        'config_name': row['config_name'], 
                        'model_path': Path(row['model_path']) # ğŸ‘ˆ strì„ Path ê°ì²´ë¡œ ë‹¤ì‹œ ë³€í™˜
                    }
                    # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² [ì—¬ê¸°ê¹Œì§€ ìˆ˜ì •] â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
                    
                    result = run_backtest_and_visualize(
                        row['ticker'], 
                        row['name'], 
                        model_info, 
                        all_trading_dates, 
                        cfg, 
                        row['predicted_close'],
                        backtest_days=20
                    )
                    result['ticker'] = row['ticker']
                    additional_backtest_results.append(result)
                
                if additional_backtest_results:
                    df_additional_backtest = pd.DataFrame(additional_backtest_results)
                    remaining_candidates = pd.merge(
                        remaining_candidates, 
                        df_additional_backtest, 
                        on='ticker', 
                        how='left'
                    )
                    
                    df_additional_strict = remaining_candidates[
                        (remaining_candidates['backtest_f1'] >= backtest_f1_threshold) & 
                        (remaining_candidates['backtest_nrmse'] <= backtest_nrmse_threshold)
                    ].copy()
                    
                    if not df_additional_strict.empty:
                        print(f"      âœ“ ì¶”ê°€ {len(df_additional_strict)}ê°œ ì¢…ëª©ì´ ì—„ê²© ê¸°ì¤€ í†µê³¼")
                        df_backtest_filtered = pd.concat([df_backtest_filtered, df_additional_strict], ignore_index=True)
        
        # 2ë‹¨ê³„: ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ì™„í™”ëœ ê¸°ì¤€ ì ìš©
        if len(df_backtest_filtered) < num_final:
            print(f"\n   ğŸ“Š ì—„ê²© ê¸°ì¤€ í†µê³¼ ì¢…ëª©({len(df_backtest_filtered)}ê°œ)ì´ ì—¬ì „íˆ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            print(f"      -> ì™„í™” ê¸°ì¤€ ì ìš©: F1 >= {relaxed_f1_threshold}, NRMSE <= {relaxed_nrmse_threshold}")
            
            all_backtested = df_recommended.copy()
            if 'remaining_candidates' in locals() and not remaining_candidates.empty:
                all_backtested = pd.concat([
                    df_recommended,
                    remaining_candidates
                ], ignore_index=True).drop_duplicates(subset=['ticker'])
            
            df_relaxed = all_backtested[
                (all_backtested['backtest_f1'] >= relaxed_f1_threshold) & 
                (all_backtested['backtest_nrmse'] <= relaxed_nrmse_threshold)
            ].copy()
            
            df_relaxed_sorted = df_relaxed.sort_values(
                by=['backtest_f1', 'backtest_nrmse'], 
                ascending=[False, True]
            )
            
            already_selected = set(df_backtest_filtered['ticker'].tolist())
            df_relaxed_additional = df_relaxed_sorted[
                ~df_relaxed_sorted['ticker'].isin(already_selected)
            ].head(num_final - len(df_backtest_filtered))
            
            if not df_relaxed_additional.empty:
                df_backtest_filtered = pd.concat([
                    df_backtest_filtered, 
                    df_relaxed_additional
                ], ignore_index=True)
                print(f"      âœ“ ì™„í™” ê¸°ì¤€ìœ¼ë¡œ {len(df_relaxed_additional)}ê°œ ì¢…ëª© ì¶”ê°€ (ì´ {len(df_backtest_filtered)}ê°œ)")
        
        # 3ë‹¨ê³„: ê·¸ë˜ë„ ë¶€ì¡±í•˜ë©´ ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì„ íƒ
        if len(df_backtest_filtered) < num_final:
            print(f"\n   âš ï¸  ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ” ì¢…ëª©ì´ {len(df_backtest_filtered)}ê°œë¿ì…ë‹ˆë‹¤.")
            print(f"      -> ë°±í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ìƒìœ„ {num_final}ê°œë¥¼ ì„ ì •í•©ë‹ˆë‹¤.")
            
            all_backtested = df_recommended.copy()
            if 'remaining_candidates' in locals() and not remaining_candidates.empty:
                all_backtested = pd.concat([
                    df_recommended,
                    remaining_candidates
                ], ignore_index=True).drop_duplicates(subset=['ticker'])
            
            df_backtest_filtered = all_backtested.sort_values(
                by=['backtest_f1', 'backtest_nrmse'], 
                ascending=[False, True]
            ).head(num_final * 2)
        
        # --- 2ë‹¨ê³„: ë°±í…ŒìŠ¤íŠ¸ F1 ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ ---
        print("\n-> 2ë‹¨ê³„: ë°±í…ŒìŠ¤íŠ¸ ì‹ ë¢°ë„(F1) ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬...")
        
        df_sorted_by_backtest = df_backtest_filtered.sort_values(
            by=['backtest_f1', 'backtest_nrmse'], 
            ascending=[False, True]
        )
        
        print(f"   - ë°±í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ìƒìœ„ ì¢…ëª©:")
        for idx, (_, row) in enumerate(df_sorted_by_backtest.head(min(10, len(df_sorted_by_backtest))).iterrows(), 1):
            print(f"      {idx}. {row['name']}({row['ticker']}): F1={row['backtest_f1']:.3f}, NRMSE={row['backtest_nrmse']:.3f}")
        
        # --- 3ë‹¨ê³„: ì„¹í„° ë¶„ì‚° í›„ ìµœì¢… ê°œìˆ˜ ì„ ì • ---
        print(f"\n-> 3ë‹¨ê³„: ìµœì¢… {num_final}ê°œ ì¢…ëª© ì„ ì •...")
        
        df_top_by_backtest = df_sorted_by_backtest.head(num_final * 2)
        
        print(f"   - ì„¹í„° ë¶„ì‚° ì ìš© (í›„ë³´: {len(df_top_by_backtest)}ê°œ)")
        df_diversified = diversify_by_sector(
            df_top_by_backtest, 
            max_per_sector=cfg.MAX_PER_SECTOR
        )
        
        df_selected = df_diversified.head(num_final)
        
        # --- 4ë‹¨ê³„: ì˜ˆìƒ ìˆ˜ìµë¥  ê¸°ì¤€ìœ¼ë¡œ ì¬ì •ë ¬ ---
        print(f"\n-> 4ë‹¨ê³„: ì˜ˆìƒ ìˆ˜ìµë¥  ê¸°ì¤€ìœ¼ë¡œ ìµœì¢… ì •ë ¬...")
        
        df_final = df_selected.sort_values('pred_trend_return', ascending=False)
        
        print(f"   âœ“ ìµœì¢… {len(df_final)}ê°œ ì¢…ëª© ì„ ì • ì™„ë£Œ")
        print(f"\n   ã€ìµœì¢… ì¶”ì²œ ì¢…ëª© (ìˆ˜ìµë¥  ìˆœ)ã€‘")
        for idx, (_, row) in enumerate(df_final.iterrows(), 1):
            print(f"      {idx}. {row['name']:15s} ({row['ticker']:6s}) | "
                  f"ì˜ˆìƒìˆ˜ìµë¥ : {row['pred_trend_return']:+6.2f}% | "
                  f"BT_F1: {row['backtest_f1']:.3f} | "
                  f"BT_NRMSE: {row['backtest_nrmse']:.3f}")
        
        if not df_final.empty:
            sector_dist = df_final['sector'].value_counts()
            print(f"\n   âœ“ ìµœì¢… ì¶”ì²œ ì¢…ëª©ì˜ ì„¹í„° ë¶„í¬:")
            for sector, count in sector_dist.items():
                print(f"      - {sector}: {count}ê°œ")
        
        df_recommended = df_final.copy()        

    # ========== Excel íŒŒì¼ ì¶œë ¥ ì½”ë“œ ì¶”ê°€ ==========
    if not df_recommended.empty:
        print("\n" + "="*20 + " STEP 6: Excel íŒŒì¼ ì €ì¥ " + "="*20)
        
        try:
            # ë‹¤ìŒ ê±°ë˜ì¼ ê³„ì‚°
            next_trading_day = get_next_trading_day(pd.to_datetime(end_date_str), all_trading_dates)
            
            # Excel ì¶œë ¥ìš© ë°ì´í„°í”„ë ˆì„ ì¤€ë¹„
            excel_df = df_recommended[['ticker', 'name', 'market', 'sector', 
                                 'last_close', 'predicted_close', 
                                 'expected_return', 'pred_trend_return',
                                 'f1', 'nrmse', 'backtest_f1', 'backtest_nrmse', 
                                 'confidence', 'avg_volume_20d']].copy()
            
            # ì˜ˆì¸¡ ë‚ ì§œ ì •ë³´ ì¶”ê°€
            excel_df.insert(0, 'ì˜ˆì¸¡ëŒ€ìƒì¼', next_trading_day.strftime('%Y-%m-%d'))
            excel_df.insert(0, 'ê¸°ì¤€ì¼', end_date_str)
            
            # ì»¬ëŸ¼ëª…ì„ í•œê¸€ë¡œ ë³€ê²½
            excel_df.columns = ['ê¸°ì¤€ì¼', 'ì˜ˆì¸¡ëŒ€ìƒì¼', 'ì¢…ëª©ì½”ë“œ', 'ì¢…ëª©ëª…', 'ì‹œì¥', 'ì„¹í„°', 
                                'í˜„ì¬ê°€', 'ì˜ˆì¸¡ê°€', 
                                'ì˜ˆìƒìˆ˜ìµë¥ (%)', 'ì¶”ì„¸ê¸°ë°˜ìˆ˜ìµë¥ (%)',
                                'ëª¨ë¸F1', 'ëª¨ë¸NRMSE', 'ë°±í…ŒìŠ¤íŠ¸F1', 'ë°±í…ŒìŠ¤íŠ¸NRMSE', 
                                'ì‹ ë¢°ë„', '20ì¼í‰ê· ê±°ë˜ëŸ‰']
            
            # ìˆ«ì í¬ë§·íŒ…
            excel_df['í˜„ì¬ê°€'] = excel_df['í˜„ì¬ê°€'].round(0).astype(int)
            excel_df['ì˜ˆì¸¡ê°€'] = excel_df['ì˜ˆì¸¡ê°€'].round(0).astype(int)
            excel_df['ì˜ˆìƒìˆ˜ìµë¥ (%)'] = excel_df['ì˜ˆìƒìˆ˜ìµë¥ (%)'].round(2)
            excel_df['ì¶”ì„¸ê¸°ë°˜ìˆ˜ìµë¥ (%)'] = excel_df['ì¶”ì„¸ê¸°ë°˜ìˆ˜ìµë¥ (%)'].round(2)
            excel_df['ëª¨ë¸F1'] = excel_df['ëª¨ë¸F1'].round(4)
            excel_df['ëª¨ë¸NRMSE'] = excel_df['ëª¨ë¸NRMSE'].round(4)
            excel_df['ë°±í…ŒìŠ¤íŠ¸F1'] = excel_df['ë°±í…ŒìŠ¤íŠ¸F1'].round(4)
            excel_df['ë°±í…ŒìŠ¤íŠ¸NRMSE'] = excel_df['ë°±í…ŒìŠ¤íŠ¸NRMSE'].round(4)
            excel_df['20ì¼í‰ê· ê±°ë˜ëŸ‰'] = excel_df['20ì¼í‰ê· ê±°ë˜ëŸ‰'].round(0).astype(int)
            
            # ì‹œì¥ ìƒí™© ì •ë³´ ì¶”ê°€
            summary_data = {
                'í•­ëª©': [
                    'ê¸°ì¤€ì¼',
                    'ì˜ˆì¸¡ëŒ€ìƒì¼',
                    'ì‹œì¥ìƒí™©', 
                    'KOSPIì˜ˆìƒìˆ˜ìµë¥ ', 
                    'KOSDAQì˜ˆìƒìˆ˜ìµë¥ ', 
                    'ì¶”ì²œì¢…ëª©ìˆ˜'
                ],
                'ê°’': [
                    end_date_str,
                    next_trading_day.strftime('%Y-%m-%d'),
                    market_sentiment, 
                    f"{df_index_pred[df_index_pred['ticker']=='KOSPI']['predicted_return'].values[0]:.2f}%" if not df_index_pred.empty and 'KOSPI' in df_index_pred['ticker'].values else 'N/A',
                    f"{df_index_pred[df_index_pred['ticker']=='KOSDAQ']['predicted_return'].values[0]:.2f}%" if not df_index_pred.empty and 'KOSDAQ' in df_index_pred['ticker'].values else 'N/A',
                    len(df_recommended)  # df_final â†’ df_recommended
                ]
            }
            summary_df = pd.DataFrame(summary_data)
            
            # Excel íŒŒì¼ë¡œ ì €ì¥ (ì—¬ëŸ¬ ì‹œíŠ¸)
            with pd.ExcelWriter(cfg.RECOMMENDATION_EXCEL_PATH, engine='openpyxl') as writer:
                # ìš”ì•½ ì •ë³´ ì‹œíŠ¸
                summary_df.to_excel(writer, sheet_name='ìš”ì•½', index=False)
                
                # ì¶”ì²œ ì¢…ëª© ì‹œíŠ¸
                excel_df.to_excel(writer, sheet_name='ì¶”ì²œì¢…ëª©', index=False)
                
                # ì‹œì¥ ì§€ìˆ˜ ì˜ˆì¸¡ ì‹œíŠ¸
                if not df_index_pred.empty:
                    df_index_pred.to_excel(writer, sheet_name='ì‹œì¥ì§€ìˆ˜ì˜ˆì¸¡', index=False)
            
            print(f"  âœ“ Excel íŒŒì¼ ì €ì¥ ì™„ë£Œ: {cfg.RECOMMENDATION_EXCEL_PATH}")
            print(f"    - íŒŒì¼ ìœ„ì¹˜: {cfg.RECOMMENDATION_EXCEL_PATH.absolute()}")
            print(f"    - ê¸°ì¤€ì¼: {end_date_str} â†’ ì˜ˆì¸¡ëŒ€ìƒì¼: {next_trading_day.strftime('%Y-%m-%d')}")
            
        except Exception as e:
            print(f"  âœ— Excel íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            print(f"    ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__}: {str(e)}")

# ========== Excel íŒŒì¼ ì¶œë ¥ ì½”ë“œ ë ==========
    end_time = datetime.now()
    print(f"\n\nğŸ ëª¨ë“  ì‘ì—… ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {end_time - start_time})")

if __name__ == '__main__':
#    test_inference()
    
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    main()
